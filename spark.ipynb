{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7206ee7b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Big Data Problem & How it Was Solved](#big-data-problem--how-it-was-solved)\n",
    "- [Scripting Languages vs Programming Interfaces in Databases](#scripting-languages-vs-programming-interfaces-in-databases)\n",
    "- [Hadoop & its Core-Compoents](#hadoop--its-core-components)\n",
    "    - [YARN (Yet Another Resource Negotiator)](#yarn)\n",
    "    - [HDFS (Hadoop Distributed File System)](#hdfs-hadoop-distributed-file-system)\n",
    "    - [MapReduce](#mapreduce)\n",
    "- [History of Hadoop,Google Big Data Solutions & hive](#history-of-hadoop-googles-big-data-solutions--hive)\n",
    "- [From Hadoop & Hive -> Spark: Problems and Solutions](#from-hadoop--hive---spark-problems-and-solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc880e",
   "metadata": {},
   "source": [
    "# Big Data Problem & How It Was Solved  \n",
    "\n",
    "In the early days, data processing was handled by COBOL (1959) and later by RDBMS systems like Oracle, SQL Server, and MySQL.  \n",
    "These systems were designed for **structured data** stored in rows and columns. They provided SQL for querying, scripting languages (PL/SQL, T-SQL), and programming interfaces (JDBC/ODBC).  \n",
    "For decades, this worked perfectly.  \n",
    "\n",
    "But then data began to change.  \n",
    "We started seeing **semi-structured formats** like JSON and XML, and soon **unstructured data** such as PDFs, images, videos, and logs.  \n",
    "Businesses were now collecting **huge amounts of data (terabytes to petabytes)** at **very high speed** from internet, mobile apps, and social media.  \n",
    "\n",
    "This shift created the **Big Data problem**, defined by the 3Vs:  \n",
    "- **Volume** â†’ massive data sizes  \n",
    "- **Velocity** â†’ fast generation and the need for real-time/near real-time processing  \n",
    "- **Variety** â†’ structured, semi-structured, and unstructured formats  \n",
    "\n",
    "Traditional RDBMS was not built to handle these challenges.  \n",
    "\n",
    "---\n",
    "\n",
    "## Approaches to Big Data  \n",
    "Two main approaches were considered:  \n",
    "\n",
    "- **Monolithic systems** (e.g., Teradata, Exadata)  \n",
    "  These relied on one huge, powerful machine. They scaled vertically (adding more CPU, RAM, disk to the same box). While powerful, they were costly, hard to scale quickly, and not fault-tolerantâ€”if hardware failed, the whole system went down.  \n",
    "\n",
    "- **Distributed systems**  \n",
    "  Instead of one large machine, this approach used a **cluster of many smaller machines** working together. They scaled horizontally (just add more machines), tolerated hardware failures (other machines kept running), and were far more economical.  \n",
    "\n",
    "---\n",
    "\n",
    "## Hadoop: The Solution ðŸš€  \n",
    "The industry adopted the distributed approach, which led to the creation of **Hadoop**.  \n",
    "\n",
    "Hadoop acted like an **operating system for a cluster of computers**. It gave three key capabilities:  \n",
    "1. **Cluster Management** â€“ making many machines work as if they were one.  \n",
    "2. **Distributed Storage** â€“ storing structured, semi-structured, and unstructured data across the cluster.  \n",
    "3. **MapReduce Processing** â€“ running programs in parallel across machines for fast data processing.  \n",
    "\n",
    "Over time, an ecosystem grew around Hadoop:  \n",
    "- **Hive** allowed SQL-style queries.  \n",
    "- **Pig** gave a scripting language.  \n",
    "- **HBase** supported NoSQL storage.  \n",
    "- **Sqoop** handled data import/export.  \n",
    "- **Oozie** managed workflows.  \n",
    "\n",
    "---\n",
    "\n",
    "## RDBMS vs Hadoop  \n",
    "RDBMS remained strong for **small to medium structured data**, but Hadoop became the go-to platform for **large-scale, diverse, fast-moving data**.  \n",
    "It could scale to petabytes, handle all data types, and process information in parallel, making Big Data manageable.  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Takeaway  \n",
    "The Big Data problem arose because of the **explosion of data in volume, velocity, and variety**.  \n",
    "RDBMS could not keep up, so the industry moved towards **distributed systems**.  \n",
    "Hadoop emerged as the practical solutionâ€”scalable, fault-tolerant, economical, and capable of handling modern data challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52262d0",
   "metadata": {},
   "source": [
    "# Scripting Languages vs Programming Interfaces in Databases\n",
    "\n",
    "## ðŸ”¹ Scripting Languages (inside the DB engine)\n",
    "**What they are:** SQL extensions that add programming constructs so logic can run *inside* the database.\n",
    "**Common ones:** **PL/SQL** (Oracle), **T-SQL** (SQL Server)  \n",
    "**Why use them:** Encapsulate business rules near the data; support transactions, error handling, jobs, and stored procedures.\n",
    "\n",
    "**Features**\n",
    "- Variables, control flow (`IF`, loops)\n",
    "- Stored procedures, functions, triggers\n",
    "- Exceptions / error handling\n",
    "- Execute multiple SQL statements as one program\n",
    "\n",
    "**Mini-example (PL/SQL)**\n",
    "```sql\n",
    "DECLARE\n",
    "  total_sales NUMBER;\n",
    "BEGIN\n",
    "  SELECT SUM(amount) INTO total_sales FROM orders WHERE order_date = TRUNC(SYSDATE);\n",
    "  IF total_sales > 10000 THEN\n",
    "    DBMS_OUTPUT.PUT_LINE('High sales day');\n",
    "  ELSE\n",
    "    DBMS_OUTPUT.PUT_LINE('Normal');\n",
    "  END IF;\n",
    "END;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Programming Interfaces (from external applications)\n",
    "**What they are:** APIs/drivers that let applications (Java, Python, C#, etc.) connect to a DB, send SQL, and read results.\n",
    "**Common ones:** **ODBC** (language-agnostic standard), **JDBC** (Java-specific)  \n",
    "**Why use them:** Build apps/services that query/update the database, integrate with UIs, APIs, and batch jobs.\n",
    "\n",
    "**Typical flow**\n",
    "1. Open a connection (URL, user, password)\n",
    "2. Prepare/execute SQL\n",
    "3. Iterate result set\n",
    "4. Close resources\n",
    "\n",
    "**Mini-example (Java + JDBC)**\n",
    "```java\n",
    "import java.sql.*;\n",
    "class Demo {\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    try (Connection c = DriverManager.getConnection(\n",
    "           \"jdbc:mysql://localhost:3306/mydb\",\"user\",\"pass\");\n",
    "         PreparedStatement ps = c.prepareStatement(\"SELECT name FROM students\");\n",
    "         ResultSet rs = ps.executeQuery()) {\n",
    "      while (rs.next()) System.out.println(rs.getString(\"name\"));\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… One-line difference to remember\n",
    "- **Scripting languages** â†’ run **inside** the database (procedures/functions/triggers).\n",
    "- **Programming interfaces** â†’ connect **from outside** so your app can talk to the DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34663cce",
   "metadata": {},
   "source": [
    "# Hadoop & Its Core Components  \n",
    "\n",
    "Hadoop is a **distributed data processing platform**. It lets you use many computers (a cluster) as if they were **one single big computer**.  \n",
    "\n",
    "It has **three main functions** (layers):  \n",
    "\n",
    "1. **YARN (Yet Another Resource Negotiator)**  \n",
    "   - Acts like the **Operating System of the cluster**.  \n",
    "   - Manages resources (CPU, RAM) across all nodes.  \n",
    "   - Decides *which app runs where*.  \n",
    "\n",
    "2. **HDFS (Hadoop Distributed File System)**  \n",
    "   - Storage layer of Hadoop.  \n",
    "   - Splits files into blocks and spreads them across multiple machines.  \n",
    "   - Keeps copies (replicas) for fault tolerance.  \n",
    "\n",
    "3. **MapReduce**  \n",
    "   - Processing layer of Hadoop.  \n",
    "   - Breaks big jobs into smaller tasks and runs them in parallel across the cluster.  \n",
    "   - â€œMapâ€ phase splits the work, â€œReduceâ€ phase combines the results.  \n",
    "\n",
    "---\n",
    "\n",
    "# YARN   \n",
    "\n",
    "YARN = **Yet Another Resource Negotiator**  \n",
    "ðŸ‘‰ Think of it as the **traffic controller** of the Hadoop cluster.  \n",
    "\n",
    "## Why YARN?  \n",
    "- On a single computer, your **OS** decides how programs share CPU and RAM.  \n",
    "- In a cluster, you need the same thing â€” a system to manage resources across **many computers**.  \n",
    "- Thatâ€™s YARNâ€™s job.  \n",
    "\n",
    "---\n",
    "\n",
    "## YARN Architecture  \n",
    "YARN has **3 main components**:  \n",
    "\n",
    "1. **Resource Manager (RM)**  \n",
    "   - Runs on the **master node**.  \n",
    "   - The global **boss** of the cluster.  \n",
    "   - Decides *which worker node will run an application*.  \n",
    "\n",
    "2. **Node Manager (NM)**  \n",
    "   - Runs on **every worker node**.  \n",
    "   - Local **site manager** for that machine.  \n",
    "   - Reports health and available CPU/RAM to RM.  \n",
    "   - Starts/stops application containers when told by RM.  \n",
    "\n",
    "3. **Application Master (AM)**  \n",
    "   - Created fresh for **each application** or **job**.  \n",
    "   - Lives inside a **container**.  \n",
    "   - Manages *just that one app*: requesting more containers, tracking tasks, reporting back.  \n",
    "\n",
    "---\n",
    "\n",
    "## Containers in YARN  \n",
    "- A **container = a mini-computer inside a worker node**.  \n",
    "- It bundles some CPU + RAM that only one app can use.  \n",
    "- Example: **2 CPU cores + 4 GB RAM** reserved for one container.  \n",
    "\n",
    "ðŸ‘‰ Multiple containers can run on the same worker node, as long as it has enough resources.  \n",
    "\n",
    "---\n",
    "\n",
    "## How an Application Runs on YARN (Step by Step)  \n",
    "\n",
    "1. **Submit Application**  \n",
    "   - You give your code (app) to the Resource Manager.  \n",
    "\n",
    "2. **Resource Manager Decides**  \n",
    "   - RM checks all Node Managers â†’ finds one with free CPU/RAM.  \n",
    "\n",
    "3. **Node Manager Starts a Container**  \n",
    "   - NM carves out a container (mini-computer) and launches an **Application Master** inside it.  \n",
    "\n",
    "4. **Application Master Runs App**  \n",
    "   - AM manages your app:  \n",
    "     - Requests more containers if needed.  \n",
    "     - Coordinates tasks across the cluster.  \n",
    "\n",
    "5. **Completion**  \n",
    "   - When tasks finish, AM informs RM.  \n",
    "   - Containers are released, resources become free.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Points to Remember  \n",
    "- **YARN = Cluster Resource Manager**.  \n",
    "- **3 components**:  \n",
    "  - RM = global boss  \n",
    "  - NM = site manager on each node  \n",
    "  - AM = per-app project manager  \n",
    "- **Container = mini-computer inside a node** (CPU + RAM slice).  \n",
    "- Each application always has at least one container (for its AM), and can request more for tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "#  HDFS (Hadoop Distributed File System)\n",
    "\n",
    "HDFS is Hadoopâ€™s **storage system**.  \n",
    "It allows you to **save large files across a cluster of computers** and **read them back later**.  \n",
    "Just like your laptop has a file system (NTFS, ext4, etc.), a Hadoop cluster has its own â†’ **HDFS**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Components of HDFS\n",
    "\n",
    "1. **NameNode (Master)**\n",
    "   - Runs on the **master node**.\n",
    "   - Stores **metadata** (information *about* the files, not the data itself).\n",
    "   - Metadata includes:\n",
    "     - File name, directory\n",
    "     - File size\n",
    "     - Number of blocks\n",
    "     - Sequence/order of blocks\n",
    "     - Which DataNodes hold which blocks\n",
    "\n",
    "2. **DataNodes (Workers)**\n",
    "   - Run on **all worker nodes**.\n",
    "   - Store the **actual file data** as blocks.\n",
    "   - Periodically report back to the NameNode about their health and stored blocks.\n",
    "\n",
    "ðŸ‘‰ Analogy:  \n",
    "- **NameNode = Librarian** (knows where every page of a book is stored).  \n",
    "- **DataNodes = Shelves** (actually hold the book pages).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ How Writing Works in HDFS\n",
    "\n",
    "1. User issues a file copy command (e.g., `hdfs dfs -put myfile.txt /data/`).\n",
    "2. Request goes to the **NameNode**.\n",
    "3. NameNode decides *which DataNodes* will store the file.\n",
    "4. File is **split into blocks** (default = 128 MB).\n",
    "5. Blocks are distributed across different DataNodes.\n",
    "\n",
    "**Example:**  \n",
    "A 512 MB file â†’ split into 4 blocks (128 MB each).  \n",
    "- Block 1 â†’ DataNode A  \n",
    "- Block 2 â†’ DataNode B  \n",
    "- Block 3 â†’ DataNode C  \n",
    "- Block 4 â†’ DataNode A (or another node)\n",
    "\n",
    "The **NameNode** keeps the â€œmapâ€ of where each block is stored.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ How Reading Works in HDFS\n",
    "\n",
    "1. User issues a **read request**.\n",
    "2. The request goes to the **NameNode**.\n",
    "3. NameNode provides:\n",
    "   - Block IDs\n",
    "   - Order of blocks\n",
    "   - Which DataNodes hold them\n",
    "4. Client fetches blocks directly from DataNodes (in parallel).\n",
    "5. Client **reassembles blocks** into the original file.\n",
    "\n",
    "ðŸ‘‰ Flow:  \n",
    "**NameNode â†’ map of file**  \n",
    "**DataNodes â†’ actual file pieces**  \n",
    "**Client â†’ puts pieces back together**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Points to Remember\n",
    "- **HDFS stores files by breaking them into blocks** (default 128 MB).\n",
    "- **NameNode stores metadata** (file info and block locations).\n",
    "- **DataNodes store actual file blocks**.\n",
    "- **Write = file split â†’ blocks sent to DataNodes**.  \n",
    "- **Read = NameNode gives map â†’ DataNodes send blocks â†’ client rebuilds file**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd56961",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "MapReduce is both:  \n",
    "1. **A Programming Model** â†’ a way to break big problems into smaller steps (Map + Reduce).  \n",
    "2. **A Programming Framework** â†’ Hadoopâ€™s system (YARN + HDFS + MapReduce engine) that runs your code across a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Programming Model (General Explanation)\n",
    "\n",
    "- **Map Stage**  \n",
    "  - Input data is split into blocks (128 MB by default).  \n",
    "  - Each block is processed independently by a `map()` function.  \n",
    "  - Mapper outputs intermediate results (e.g., partial counts).\n",
    "\n",
    "- **Reduce Stage**  \n",
    "  - Collects outputs from all mappers.  \n",
    "  - Aggregates them with a `reduce()` function.  \n",
    "  - Produces the final result.  \n",
    "\n",
    "ðŸ‘‰ **Map = parallel block-level work**  \n",
    "ðŸ‘‰ **Reduce = aggregation/consolidation**\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Programming Framework (General Explanation)\n",
    "\n",
    "- **Resource Manager (RM)**  \n",
    "  - Runs on the master node.  \n",
    "  - Global cluster boss.  \n",
    "  - Decides how many containers each job gets.  \n",
    "\n",
    "- **Application Master (AM)**  \n",
    "  - Runs once per job (inside a container on a worker node).  \n",
    "  - Job-level boss.  \n",
    "  - Requests containers from RM, launches tasks, monitors them.  \n",
    "\n",
    "- **Map Task**  \n",
    "  - One per block.  \n",
    "  - Runs the userâ€™s `map()` function.  \n",
    "  - Processes actual data.  \n",
    "\n",
    "- **Reduce Task**  \n",
    "  - Collects mapper outputs.  \n",
    "  - Runs userâ€™s `reduce()` function.  \n",
    "  - Aggregates results into final output.  \n",
    "\n",
    "ðŸ‘‰ **Flow**:  \n",
    "Client â†’ RM â†’ AM â†’ Map Tasks â†’ Reduce Task â†’ Final Output  \n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Example: Counting Lines in a 20 TB File\n",
    "\n",
    "### Step A â€“ Storage in HDFS\n",
    "- File size = **20 TB**  \n",
    "- Block size = **128 MB**  \n",
    "- Number of blocks = 20 TB Ã· 128 MB â‰ˆ **160,000 blocks**  \n",
    "- HDFS spreads blocks across 20 worker nodes.  \n",
    "\n",
    "### Step B â€“ MapReduce Execution\n",
    "1. **Job submission**  \n",
    "   - Client submits code with `map()` and `reduce()`.  \n",
    "\n",
    "2. **RM allocates AM**  \n",
    "   - RM launches **1 Application Master** for this job.  \n",
    "\n",
    "3. **Map phase**  \n",
    "   - AM requests ~160,000 containers (one per block).  \n",
    "   - Each container runs a Map task to count lines in its block.  \n",
    "   - Outputs partial counts:  \n",
    "     - Block 1 â†’ 1.2M lines  \n",
    "     - Block 2 â†’ 1.3M lines â€¦  \n",
    "\n",
    "4. **Reduce phase**  \n",
    "   - After all maps finish, AM requests a container for the Reduce task.  \n",
    "   - Reducer collects 160,000 partial counts.  \n",
    "   - Aggregates them â†’ final line count for the 20 TB file.  \n",
    "\n",
    "5. **Result**  \n",
    "   - Output is stored back in HDFS or returned to the client.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Roles Recap (General + Example)\n",
    "\n",
    "| Component | General Role | 20 TB Example |\n",
    "|-----------|--------------|----------------|\n",
    "| **RM** | Global cluster resource allocator | Gave containers for ~160,000 Map tasks + 1 Reduce task |\n",
    "| **AM** | Job coordinator | Managed this line-count job, monitored Map + Reduce tasks |\n",
    "| **Map Task** | Processes one block with `map()` | Each of 160,000 tasks counted lines in one block |\n",
    "| **Reduce Task** | Aggregates mapper outputs | Summed 160,000 partial counts into one total |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "- **General:** MapReduce splits work into `map()` (parallel per block) and `reduce()` (aggregation).  \n",
    "- **Framework:** RM (cluster boss), AM (job boss), Map tasks (block workers), Reduce tasks (final aggregator).  \n",
    "- **20 TB Example:** 160,000 Map tasks count lines block-by-block â†’ Reduce task adds them â†’ total line count produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f83f9",
   "metadata": {},
   "source": [
    "# History of Hadoop, Googleâ€™s Big Data Solutions & Hive\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Googleâ€™s Big Data Problem\n",
    "\n",
    "In the early 2000s, Google needed to build a **web search engine**.  \n",
    "They faced four massive challenges:\n",
    "\n",
    "1. **Data Collection (Web Crawling)**\n",
    "   - Problem: Needed to fetch billions of web pages from the internet.  \n",
    "   - Solution: Built large-scale **web crawlers** to download HTML + metadata.\n",
    "\n",
    "2. **Data Storage (Petabyte Scale)**\n",
    "   - Problem: Too much data for a single machine â†’ traditional databases failed.  \n",
    "   - Solution: Built the **Google File System (GFS, 2003)**.  \n",
    "     - Split files into chunks (64 MB).  \n",
    "     - Distributed across many cheap machines.  \n",
    "     - Replicated chunks for fault tolerance.\n",
    "\n",
    "3. **Data Processing (Computation Power)**\n",
    "   - Problem: Needed to run algorithms (like **PageRank**) on billions of pages + links.  \n",
    "   - Solution: Created the **MapReduce model (2004)**.  \n",
    "     - **Map** â†’ process small chunks of data in parallel.  \n",
    "     - **Reduce** â†’ aggregate results.  \n",
    "     - Allowed processing huge datasets in hours instead of months.\n",
    "\n",
    "4. **Data Serving (Querying Results)**\n",
    "   - Problem: Users needed instant answers to queries (â€œbest pizza in New Yorkâ€).  \n",
    "   - Solution: Built **BigTable** (distributed, column-oriented DB).  \n",
    "     - Stored **inverted index**: words â†’ list of pages containing them.  \n",
    "     - Supported **random access lookups** across thousands of machines.  \n",
    "     - Combined with PageRank â†’ returned ranked results in milliseconds.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. From Google â†’ Hadoop\n",
    "\n",
    "- Google published **whitepapers**:\n",
    "  - **2003 â†’ GFS** (storage)\n",
    "  - **2004 â†’ MapReduce** (processing)\n",
    "\n",
    "- The **open-source community (Doug Cutting & Mike Cafarella)** created an open-source version:\n",
    "  - **HDFS (Hadoop Distributed File System)** â†’ based on GFS.  \n",
    "  - **Hadoop MapReduce Framework** â†’ based on Googleâ€™s MapReduce.\n",
    "\n",
    "ðŸ‘‰ Thus, **Hadoop was born**: an open-source big data platform inspired by Googleâ€™s systems.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Hive: Making Hadoop Easier\n",
    "\n",
    "- Problem: Writing **raw MapReduce jobs in Java** was **hard** and time-consuming.  \n",
    "- Solution: Facebook built **Hive** (later Apache Hive).  \n",
    "\n",
    "**Hive provided:**\n",
    "1. Ability to create **databases, tables, and views** on Hadoop data.  \n",
    "2. A **SQL-like language (HiveQL)** to query big datasets.  \n",
    "3. An engine that **translated SQL â†’ MapReduce jobs** internally.  \n",
    "\n",
    "ðŸ‘‰ Developers familiar with SQL could now use Hadoop **without writing MapReduce code**.  \n",
    "ðŸ‘‰ Example:\n",
    "```sql\n",
    "SELECT COUNT(*) \n",
    "FROM logs \n",
    "WHERE status = 'ERROR';\n",
    "```\n",
    "Hive would turn this into a MapReduce workflow under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- **Googleâ€™s Innovations**\n",
    "  - **GFS â†’ storage system** for petabytes.  \n",
    "  - **MapReduce â†’ processing model** for parallel computation.  \n",
    "  - **BigTable â†’ fast query engine** for serving results.  \n",
    "\n",
    "- **Hadoopâ€™s Implementations**\n",
    "  - **HDFS** (open-source GFS).  \n",
    "  - **Hadoop MapReduce Framework** (open-source MapReduce).  \n",
    "  - **HBase** (open-source version of BigTable).  \n",
    "\n",
    "- **Hiveâ€™s Role**\n",
    "  - Simplified Hadoop usage with SQL-like queries.  \n",
    "  - Made Hadoop accessible to the broader developer community.\n",
    "\n",
    "ðŸ‘‰ Hadoop = **Googleâ€™s ideas, made open-source**.  \n",
    "ðŸ‘‰ Hive = **SQL on Hadoop**, making big data processing much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0da2cb",
   "metadata": {},
   "source": [
    "# From Hadoop & Hive -> Spark: Problems and Solutions\n",
    "\n",
    "Hadoop (HDFS + MapReduce) and Hive were revolutionary,  \n",
    "but they faced critical limitations. Apache Spark was created to fix them.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Performance (Speed)\n",
    "\n",
    "**Problem with Hadoop/Hive**  \n",
    "- Hadoop MapReduce wrote **intermediate results to disk** after every stage.  \n",
    "- Heavy disk I/O slowed jobs down (hours for large datasets).  \n",
    "- Hive SQL queries were also slow because they ran on MapReduce engine.  \n",
    "\n",
    "**Sparkâ€™s Solution**  \n",
    "- Spark keeps intermediate data **in memory (RAM)** whenever possible.  \n",
    "- Only falls back to disk if needed.  \n",
    "- Result: **10xâ€“100x faster** than MapReduce/Hive for most workloads.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Programming Complexity\n",
    "\n",
    "**Problem with Hadoop**  \n",
    "- Writing MapReduce programs in Java was complex and verbose.  \n",
    "- Even simple word count required dozens of lines of Java.  \n",
    "- Hive simplified queries with SQL, but still ran on slow MapReduce.  \n",
    "\n",
    "**Sparkâ€™s Solution**  \n",
    "- Spark introduced **high-level, composable APIs**:  \n",
    "  - `.map()`, `.filter()`, `.reduceByKey()`, `.join()`, `.groupBy()`  \n",
    "- Provided **Spark SQL** for SQL queries.  \n",
    "- Easier to write and much faster to run. \n",
    "- big thanks to in memory computing \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Language Support\n",
    "\n",
    "**Problem with Hadoop/Hive**  \n",
    "- Hadoop MapReduce was **Java-only**.  \n",
    "- Hive SQL was easier, but lacked flexibility for advanced tasks (ML, streaming).  \n",
    "\n",
    "**Sparkâ€™s Solution**  \n",
    "- Multi-language support: **Scala, Java, Python (PySpark), R (SparkR)**.  \n",
    "- Opened big data processing to data scientists and analysts, not just Java engineers.  \n",
    "\n",
    "\n",
    "\n",
    "Hadoop MapReduce was **Java-only** and rigid.  \n",
    "- Writing iterative algorithms for **machine learning** was painful: each iteration became a new MapReduce job, with results written to **disk** every time â†’ extremely slow.  \n",
    "- Hadoop also couldnâ€™t handle **streaming data** (only batch processing).  \n",
    "\n",
    "Spark solved this with **multi-language support** (Scala, Java, Python, R) and an **in-memory engine**.  \n",
    "- ML algorithms reuse data in **RAM** across iterations â†’ 10â€“100x faster than Hadoop.  \n",
    "- Spark introduced **Structured Streaming**, enabling near real-time analytics, impossible in Hadoopâ€™s disk-based batch model.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "| Feature              | Hadoop/Hive                         | Spark                                      |\n",
    "|----------------------|-------------------------------------|--------------------------------------------|\n",
    "| Language Support     | Java-only (MapReduce), SQL (Hive)   | Scala, Java, Python, R                     |\n",
    "| Machine Learning     | Very slow (iterative jobs to disk)  | Fast, in-memory MLlib                      |\n",
    "| Streaming            | Not supported (batch only)          | Near real-time (Spark Streaming)           |\n",
    "| API Flexibility      | Rigid Map â†’ Reduce only             | Rich APIs (map, filter, join, SQL, ML, etc)|\n",
    "\n",
    "ðŸ‘‰ Spark broadened big data beyond Java engineers â†’ accessible to **analysts, data scientists, and ML engineers**.\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Storage Limitations\n",
    "\n",
    "**Problem with Hadoop (HDFS)**  \n",
    "- HDFS storage was tightly coupled with compute.  \n",
    "- To add storage, you had to add new Hadoop nodes (CPU + RAM you might not need).  \n",
    "- With the rise of cloud, HDFS became costly compared to **Amazon S3, Azure Blob, GCS**.  \n",
    "\n",
    "**Sparkâ€™s Solution**  \n",
    "- Spark decoupled compute from storage.  \n",
    "- Works with both **HDFS** and **cloud storage**.  \n",
    "- Companies could adopt **cloud-native architectures (Lakehouse, Data Lake)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Resource Management\n",
    "\n",
    "**Problem with Hadoop (YARN)**  \n",
    "- YARN was the only resource manager.  \n",
    "- Felt heavy compared to modern options (Docker, Mesos, Kubernetes).  \n",
    "- Limited flexibility for cloud environments.  \n",
    "\n",
    "**Sparkâ€™s Solution**  \n",
    "- Spark is **resource-manager agnostic**.  \n",
    "- Can run on **YARN, Mesos, Kubernetes, or standalone clusters**.  \n",
    "- Fits naturally into **cloud + container-native ecosystems**.  \n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Summary\n",
    "\n",
    "| Problem in Hadoop/Hive | Sparkâ€™s Solution |\n",
    "|-------------------------|------------------|\n",
    "| Slow (disk I/O bottlenecks) | In-memory computing â†’ 10â€“100x faster |\n",
    "| Complex MapReduce programming | High-level APIs + Spark SQL |\n",
    "| Java-only, limited flexibility | Multi-language (Scala, Python, R, Java) |\n",
    "| HDFS tied to compute nodes | Works with HDFS + cloud storage (S3, Blob, GCS) |\n",
    "| YARN-only resource manager | Runs on YARN, Mesos, Kubernetes, standalone |\n",
    "\n",
    "ðŸ‘‰ Hadoop was **revolutionary** for big data,  \n",
    "but Spark made it **faster, simpler, and cloud-ready**,  \n",
    "which is why Spark is now the dominant big data engine."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
