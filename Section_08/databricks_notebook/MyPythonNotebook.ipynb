{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea80596-8198-4c95-8354-9ceb92dab08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb37667-75e2-4ea4-9b3a-e32b8c5b40db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Function Explanation: `to_date_df`\n",
    "\n",
    "The `to_date_df` function is designed to convert a string column in a PySpark DataFrame to a date type. It takes three arguments:\n",
    "\n",
    "- `df`: The input DataFrame.\n",
    "- `fmt`: The date format string that matches the format of the string dates in the column.\n",
    "- `fld`: The name of the column to convert.\n",
    "\n",
    "The function uses PySpark's `to_date` function to parse the string column according to the specified format and returns a new DataFrame with the column converted to `DateType`.\n",
    "\n",
    "**Function Definition:**\n",
    "<pre>\n",
    "python\n",
    "def to_date_df(df, fmt, fld):\n",
    "    return df.withColumn(fld, to_date(col(fld), fmt))\n",
    "<pre>\n",
    "\n",
    "**Usage Example:**\n",
    "Suppose you have a DataFrame with a column `\"date_str\"` containing dates as strings in the format `\"yyyy/MM/dd\"`. You can convert this column to date type as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1879ae-f78c-422f-a278-cb1fecbe8296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_date_df(df,fmt,fld):\n",
    "    return df.withColumn(fld,to_date(col(fld),fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e51aa9-36b0-4c27-9190-26d6598f971a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType([\n",
    "    StructField(\"ID\",StringType()),\n",
    "    StructField(\"EventDate\",StringType())\n",
    "])\n",
    "\n",
    "my_rows = [Row(\"122\",\"04/05/2020\"),Row(\"123\",\"04/06/2020\"),Row(\"124\",\"04/07/2020\"),Row(\"125\",\"04/08/2020\")]\n",
    "my_df =spark.createDataFrame(my_rows,my_schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4b3196-b26d-4078-83c6-6c55c46b2460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ID: string (nullable = true)\n |-- EventDate: string (nullable = true)\n\n+---+----------+\n| ID| EventDate|\n+---+----------+\n|122|04/05/2020|\n|123|04/06/2020|\n|124|04/07/2020|\n|125|04/08/2020|\n+---+----------+\n\nroot\n |-- ID: string (nullable = true)\n |-- EventDate: date (nullable = true)\n\n+---+----------+\n| ID| EventDate|\n+---+----------+\n|122|2020-04-05|\n|123|2020-04-06|\n|124|2020-04-07|\n|125|2020-04-08|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "my_df.printSchema()\n",
    "my_df.show()\n",
    "\n",
    "new_df = to_date_df(my_df,\"MM/dd/yyyy\",\"EventDate\") \n",
    "new_df.printSchema()\n",
    "new_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MyPythonNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}