{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2557754b",
   "metadata": {},
   "source": [
    "## PySpark `.toDF()` — When to Use\n",
    "\n",
    "**What it is**\n",
    "- A convenience method that either:\n",
    "  - **Converts an RDD → DataFrame** while assigning column names, or\n",
    "  - **Renames all columns** of an existing DataFrame in one call.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Use Case 1 — RDD → DataFrame (set column names)\n",
    "    rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\")])\n",
    "    df  = rdd.toDF([\"id\", \"name\"])\n",
    "    df.show()\n",
    "**Why:** You started with an **RDD** and want to switch to the **DataFrame API** with readable column names.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Use Case 2 — Rename *all* columns of an existing DataFrame\n",
    "    data = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "    df   = spark.createDataFrame(data, [\"c1\", \"c2\"])\n",
    "    df2  = df.toDF(\"id\", \"name\")   # renames ALL columns\n",
    "    df2.show()\n",
    "**Why:** You already have a DataFrame and want to **replace every column name** at once (the count must match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e368034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark \n",
    "from pyspark.sql import * \n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "from lib.logger import Log4J\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .appName(\"MiscDemo\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "logger = Log4J(spark)\n",
    "\n",
    "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
    "                (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
    "                (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
    "                (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
    "                (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981 \n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF(\"name\",\"day\",\"month\",\"year\").repartition(3)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afefe0",
   "metadata": {},
   "source": [
    "### Spark — Short & Simple\n",
    "\n",
    "**`monotonically_increasing_id()`**\n",
    "- Gives each row a **unique 64-bit ID**.\n",
    "- IDs **increase within each partition**, not globally consecutive.\n",
    "- Expect **gaps** and **different values** if partitioning/order changes.\n",
    "- Use for **surrogate keys / uniqueness**, not for strict ordering.\n",
    "\n",
    "**`withColumn(colName, colExpr)`**\n",
    "- **Creates or replaces** a column.\n",
    "- First arg = **target column name**; second arg = **expression** that computes its values (a Spark Column expression).\n",
    "- Works **row-wise**, lazily; returns a **new DataFrame** (original unchanged).\n",
    "- Use for deriving/cleaning values; **not** for renaming (use `withColumnRenamed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755f43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|  81|          1|\n",
      "|Abdul| 23|    5|  81| 8589934592|\n",
      "| John| 12|   12|   6|17179869184|\n",
      "| Rosy|  7|    8|  63|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = raw_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fb209",
   "metadata": {},
   "source": [
    "### Spark — `expr()` & `.cast()` (short & simple)\n",
    "\n",
    "- **`expr(\"...\")`**: Write **SQL** on columns inside the DataFrame API.  \n",
    "  Returns a **Column** you can use in `select`, `withColumn`, `orderBy`, etc.  \n",
    "  *Think:* “SQL logic here.” Example idea: `CASE WHEN`, `to_date(...)`, `colA + 1`.\n",
    "\n",
    "- **`.cast(type)`**: Change a column’s **data type**.  \n",
    "  Accepts `\"int\"`, `\"double\"`, `\"string\"`, `\"date\"`, `\"timestamp\"`, or Spark types like `IntegerType()`.  \n",
    "  Non-convertible values → **NULL**.  \n",
    "  *Think:* “Make this column int/double/string now.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec1c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|1981|          1|\n",
      "|Abdul| 23|    5|1981| 8589934592|\n",
      "| John| 12|   12|2006|17179869184|\n",
      "| Rosy|  7|    8|1963|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"year\",expr(\"\"\"\n",
    "        case when year < 21 then year + 2000\n",
    "        when year < 100 then year + 1900\n",
    "        else year \n",
    "        end \"\"\").cast(IntegerType()))\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cfc52",
   "metadata": {},
   "source": [
    "### Spark — Set Data Types **Before** Transformations (short & simple)\n",
    "\n",
    "- **Define schema on read:** Provide a `StructType` via `.schema(...)` when loading CSV/JSON; avoid relying on `inferSchema`. Parquet/ORC already carry types, but still normalize if needed.\n",
    "- **Normalize early (cast once):** Right after load, convert columns to their **final types** (numbers, booleans, `date`/`timestamp` via `to_date`/`to_timestamp`). Do it **once** so you don’t keep re-casting later.\n",
    "- **Centralize the typing step:** Group all casts/parsing in one place (immediately post-ingest) to keep downstream transforms clean and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bdff760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|  81|          1|\n",
      "|Abdul| 23|    5|  81| 8589934592|\n",
      "| John| 12|   12|   6|17179869184|\n",
      "| Rosy|  7|    8|  63|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|1981|          1|\n",
      "|Abdul| 23|    5|1981| 8589934592|\n",
      "| John| 12|   12|2006|17179869184|\n",
      "| Rosy|  7|    8|1963|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.printSchema()\n",
    "df5 = df1.withColumn(\"day\",col(\"day\").cast(IntegerType())) \\\n",
    "        .withColumn(\"month\",col(\"month\").cast(IntegerType())) \\\n",
    "        .withColumn(\"year\",col(\"year\").cast(IntegerType())) \n",
    "\n",
    "df6 = df5.withColumn(\"year\",expr(\"\"\"\n",
    "            case when year < 21 then year + 2000 \n",
    "            when year < 100 then year + 1900 \n",
    "            else year \n",
    "\n",
    "            end \"\"\"))\n",
    "df6.show()\n",
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a1c44",
   "metadata": {},
   "source": [
    "### Spark — `when()`, `otherwise()`, and `col()` (short & simple)\n",
    "\n",
    "- **`when(condition, value)`**  \n",
    "  Used for conditional logic (like SQL `CASE WHEN`).  \n",
    "  Takes two args:  \n",
    "  1. A condition (built with `col(...)`, comparisons, etc.).  \n",
    "  2. The value to assign if the condition is true.\n",
    "\n",
    "- **`otherwise(value)`**  \n",
    "  Defines the fallback value if **none of the `when()` conditions match**.  \n",
    "  Works like SQL `ELSE`.\n",
    "\n",
    "- **`col(\"colName\")`**  \n",
    "  Tells Spark you are referencing a **column** (not a string).  \n",
    "  Needed inside expressions to point to a DataFrame field.\n",
    "\n",
    "---\n",
    "\n",
    "**Mental model**  \n",
    "- Use `col(\"x\")` to reference a column.  \n",
    "- Chain `when(...).when(...).otherwise(...)` to build conditional columns.  \n",
    "- It’s the non-SQL way of writing `CASE WHEN ... ELSE ... END`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57506d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|1981|          1|\n",
      "|Abdul| 23|    5|1981| 8589934592|\n",
      "| John| 12|   12|2006|17179869184|\n",
      "| Rosy|  7|    8|1963|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df6.withColumn(\"year\", \\\n",
    "            when(col(\"year\") < 21, col(\"year\") + 2000 ) \\\n",
    "            .when(col(\"year\")< 100, col(\"year\") + 1900) \\\n",
    "            .otherwise(col(\"year\"))\n",
    "            )\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9d1751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark \n",
    "from pyspark.sql import * \n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * \n",
    "from lib.logger import Log4J\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .appName(\"MiscDemo\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "logger = Log4J(spark)\n",
    "\n",
    "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
    "                (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
    "                (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
    "                (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
    "                (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981 \n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF(\"name\",\"day\",\"month\",\"year\").repartition(3)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd08b6",
   "metadata": {},
   "source": [
    "### Spark — `to_date()` with `expr()` (short & simple)\n",
    "\n",
    "- **Purpose:** Convert a string column into a proper **date** type.  \n",
    "- **Arguments (2):**  \n",
    "  1. The **string expression/column** that holds the date text.  \n",
    "  2. The **format pattern** that tells Spark how to interpret that text.  \n",
    "\n",
    "- **Format examples:**  \n",
    "  - `\"dd/MM/yyyy\"` → `15/08/1995`  \n",
    "  - `\"MM-dd-yyyy\"` → `08-15-1995`  \n",
    "  - `\"yyyyMMdd\"`   → `19950815`  \n",
    "\n",
    "- **Why format is needed:** Raw strings can look different (`\"15/08/1995\"` vs `\"1995-08-15\"`).  \n",
    "  The format tells Spark **what each part (day, month, year) means** so parsing is correct.  \n",
    "\n",
    "- **Mental model:**  \n",
    "  `to_date(\"string_column\", \"format\")` → “Read this text as a date using this format.”  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37d03d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+----------+\n",
      "| name|day|month|year|         id|       dob|\n",
      "+-----+---+-----+----+-----------+----------+\n",
      "| Ravi| 28|    1|2002|          0|2002-01-28|\n",
      "|Abdul| 23|    5|1981|          1|1981-05-23|\n",
      "|Abdul| 23|    5|1981| 8589934592|1981-05-23|\n",
      "| John| 12|   12|2006|17179869184|2006-12-12|\n",
      "| Rosy|  7|    8|1963|17179869185|1963-08-07|\n",
      "+-----+---+-----+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = df7.withColumn(\"dob\",expr(\"to_date(concat(day,'/',month,'/',year),'d/M/y')\")) \n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5bad77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+----------+\n",
      "| name|day|month|year|         id|       dob|\n",
      "+-----+---+-----+----+-----------+----------+\n",
      "| Ravi| 28|    1|2002|          0|2002-01-28|\n",
      "|Abdul| 23|    5|1981|          1|1981-05-23|\n",
      "|Abdul| 23|    5|1981| 8589934592|1981-05-23|\n",
      "| John| 12|   12|2006|17179869184|2006-12-12|\n",
      "| Rosy|  7|    8|1963|17179869185|1963-08-07|\n",
      "+-----+---+-----+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df7.withColumn(\"dob\",to_date(expr(\"concat(day,'/',month,'/',year)\"),'d/M/y'))\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2e5db",
   "metadata": {},
   "source": [
    "### Spark — Dropping Columns & Duplicates (short & simple)\n",
    "\n",
    "- **`.drop(\"colName\", \"colName2\", …)`**  \n",
    "  Removes the given columns from the DataFrame.  \n",
    "  *Think:* “Delete these columns from my table.”\n",
    "\n",
    "- **`.dropDuplicates([\"col1\", \"col2\", …])`**  \n",
    "  Removes rows where all the listed columns have the same values.  \n",
    "  Keeps the **first occurrence** and drops the rest.  \n",
    "  *Think:* “Only keep unique rows based on these columns.”  \n",
    "\n",
    "- **Mental model:**  \n",
    "  - `.drop()` → remove **unwanted columns**.  \n",
    "  - `.dropDuplicates()` → remove **duplicate rows** based on column values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4971c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+\n",
      "| name|         id|       dob|\n",
      "+-----+-----------+----------+\n",
      "| Rosy|17179869185|1963-08-07|\n",
      "|Abdul|          1|1981-05-23|\n",
      "| Ravi|          0|2002-01-28|\n",
      "| John|17179869184|2006-12-12|\n",
      "+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df7.withColumn(\"dob\",to_date(expr(\"concat(day,'/',month,'/',year)\"),'d/M/y')) \\\n",
    "        .drop(\"day\",\"month\",\"year\") \\\n",
    "        .dropDuplicates([\"name\",\"dob\"]) \\\n",
    "        .sort(expr(\"dob desc\"))\n",
    "df9.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hellospark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
