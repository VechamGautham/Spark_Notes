{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280caf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b1051",
   "metadata": {},
   "source": [
    "# Column Transformation Methods\n",
    "\n",
    "In this notebook, we will understand column transformation methods:\n",
    "\n",
    "1. `withColumnRenamed`\n",
    "2. `withColumn`\n",
    "3. `select`\n",
    "4. `where`\n",
    "5. `groupBy`\n",
    "6. `count`\n",
    "7. `orderBy`\n",
    "\n",
    "We will also cover actions and functions:\n",
    "\n",
    "- Actions: `show`, `count`\n",
    "- Functions: `display`, `expr`, `printSchema`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True) \n",
    "    .load(\"/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb19a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fire_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911720b3",
   "metadata": {},
   "source": [
    "## Column Name are not standardized\n",
    "To create a new transformed DataFrame with renamed columns, use the `.withColumnRenamed` method. Provide the current column name and the new name you want to assign. Follow the code below:\n",
    "- You can create a chain of spark transformation methods one after the other.\n",
    "- Spark transformation returns a new dataframe after transforming the old dataframe \n",
    "- spakr dataframe are immutalbe you cannot change a existing dataframe but can make changes by creating a new dataframe copying old dataframe date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_fire_df = (\n",
    "    fire_df\n",
    "    .withColumnRenamed(\"Call Number\", \"CallNumber\")\n",
    "    .withColumnRenamed(\"Unit ID\", \"UnitID\")\n",
    "    .withColumnRenamed(\"Incident Number\", \"IncidentNumber\")\n",
    "    .withColumnRenamed(\"Call Type\", \"CallType\")\n",
    "    .withColumnRenamed(\"Call Date\", \"CallDate\")\n",
    "    .withColumnRenamed(\"Watch Date\", \"WatchDate\")\n",
    "    .withColumnRenamed(\"Call Final Disposition\", \"CallFinalDisposition\")\n",
    "    .withColumnRenamed(\"Available DtTm\", \"AvailableDtTm\")\n",
    "    .withColumnRenamed(\"Address\", \"Address\")\n",
    "    .withColumnRenamed(\"City\", \"City\")\n",
    "    .withColumnRenamed(\"Zipcode of Incident\", \"Zipcode\")\n",
    "    .withColumnRenamed(\"Battalion\", \"Battalion\")\n",
    "    .withColumnRenamed(\"Station Area\", \"StationArea\")\n",
    "    .withColumnRenamed(\"Box\", \"Box\")\n",
    "    .withColumnRenamed(\"OrigPriority\", \"OrigPriority\")\n",
    "    .withColumnRenamed(\"Priority\", \"Priority\")\n",
    "    .withColumnRenamed(\"Final Priority\", \"FinalPriority\")\n",
    "    .withColumnRenamed(\"ALS Unit\", \"ALSUnit\")\n",
    "    .withColumnRenamed(\"Call Type Group\", \"CallTypeGroup\")\n",
    "    .withColumnRenamed(\"NumAlarms\", \"NumAlarms\")\n",
    "    .withColumnRenamed(\"Unit Type\", \"UnitType\")\n",
    "    .withColumnRenamed(\"Unit sequence in call dispatch\", \"UnitSequenceInCallDispatch\")\n",
    "    .withColumnRenamed(\"Fire Prevention District\", \"FirePreventionDistrict\")\n",
    "    .withColumnRenamed(\"Supervisor District\", \"SupervisorDistrict\")\n",
    "    .withColumnRenamed(\"Neighborhood\", \"Neighborhood\")\n",
    "    .withColumnRenamed(\"Location\", \"Location\")\n",
    "    .withColumnRenamed(\"Row ID\", \"RowID\")\n",
    "    .withColumnRenamed(\"Delay\", \"Delay\")\n",
    ")\n",
    "display(renamed_fire_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae26299",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f9ff4",
   "metadata": {},
   "source": [
    "## Understanding DataFrame Schema\n",
    "The `.printSchema()` method shows you the schema of the DataFrame, allowing you to understand the data types of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5444cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_fire_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef87b3",
   "metadata": {},
   "source": [
    "## Using .withColumn for Transformations\n",
    "The `.withColumn` method is a transformation that allows you to apply specific logic to a column in a DataFrame. It takes the column name and the logic to change the column, which can include altering the data type or rounding values. This method returns a new DataFrame with the transformed column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = (\n",
    "    renamed_fire_df\n",
    "    .withColumn(\"AvailableDtTm\", to_timestamp(\"AvailableDtTm\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .withColumn(\"Delay\", round(\"Delay\",2))\n",
    ")\n",
    "\n",
    "display(fire_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d172cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2dde2d",
   "metadata": {},
   "source": [
    "Q1. How many distinct types of calls were made to the Fire Department\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858181b",
   "metadata": {},
   "source": [
    "## Caching DataFrames\n",
    "The `.cache()` method helps in storing the DataFrame in memory, which allows for faster querying later. By caching the DataFrame, subsequent actions on the DataFrame can be executed more quickly as the data is retrieved from memory rather than being recomputed or read from the original source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df9441",
   "metadata": {},
   "source": [
    "#### Q1. How many distinct types of calls were made to the Fire Department ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1613b9f",
   "metadata": {},
   "source": [
    "To run queries for a DataFrame using SQL, you first need to convert the DataFrame into a temporary view and then call a SparkSession and use the `sql` method to query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73970026",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.createOrReplaceTempView(\"fire_service_calls_view\")\n",
    "\n",
    "q1_sql_df = spark.sql(\"\"\"\n",
    "            select count(distinct calltype) as distinct_call_type_count \n",
    "            from fire_service_calls_view \n",
    "            where calltype is not null      \n",
    "                \"\"\") \n",
    "\n",
    "display(q1_sql_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae2b54",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cad9af",
   "metadata": {},
   "source": [
    "dataframe logic for 1Q\n",
    "1. Filter the records and take only those where calltype is not null \n",
    "2. select the calltype column \n",
    "3. take only distinct calltypes \n",
    "4. show the count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63071ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = (fire_df\n",
    "         .where(\"CallType is not null\")\n",
    "         .select(\"CallType\")\n",
    "         .distinct()\n",
    "         )\n",
    "print(q1_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62343da",
   "metadata": {},
   "source": [
    "## Spark DataFrame Transformation Methods\n",
    "\n",
    "The `.where`, `.select`, and `.distinct` methods are essential transformation methods in Spark DataFrames. These methods allow you to filter, select, and retrieve distinct rows from your DataFrame, respectively. Each transformation returns a new DataFrame, as Spark DataFrames are immutable. Here is a brief overview of these methods:\n",
    "\n",
    "- `.where(condition)`: Filters rows based on the given condition.\n",
    "- `.select(*cols)`: Selects a subset of columns or expressions.\n",
    "- `.distinct()`: Returns a new DataFrame with distinct rows.\n",
    "\n",
    "These methods can be chained together to perform complex transformations efficiently.\n",
    "\n",
    "## Spark DataFrame Action Methods\n",
    "\n",
    "The `.count()` method is an action that triggers the execution of the Spark job and returns the count of rows to the Spark driver. It is used to get the number of elements in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df1 = fire_df.where(\"CallType is not null\")\n",
    "q1_df2 = q1_df1.select(\"CallType\")\n",
    "q1_df3 = q1_df2.distinct()\n",
    "print(q1_df3.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae59bb9",
   "metadata": {},
   "source": [
    "#### Q2. What were distinct types of calls made to the Fire Department ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8261ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.createOrReplaceTempView(\"fire_service_calls_view\")\n",
    "\n",
    "q2_sql_df = spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct calltype as distinct_call_type \n",
    "    from fire_service_calls_view \n",
    "    where calltype is not null\n",
    "\"\"\")\n",
    "\n",
    "display(q2_sql_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f27d1",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. filter the calltypes where calltype is not null\n",
    "2. select calltype \n",
    "3. take only distinct calltype \n",
    "4. show the call type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df = (\n",
    "  fire_df.where(\"CallType is not null\")\n",
    "  .select(expr(\"CallType as distinct_call_type\"))\n",
    "  .distinct()\n",
    ")\n",
    "\n",
    "display(q2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d3ff5",
   "metadata": {},
   "source": [
    "#### Q3. Find out all response for delayed times greater than 5 mins ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1db9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.createOrReplaceTempView(\"fire_service_calls_view\")\n",
    "\n",
    "q3_sql_df = spark.sql(\n",
    "    \"\"\"\n",
    "    select *\n",
    "    from fire_service_calls_view\n",
    "    where delay > 5 \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "display(q3_sql_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa94cb",
   "metadata": {},
   "source": [
    "data frame logic \n",
    "1. take only rows where delay > 5min\n",
    "2. filter the rows where delay is not null \n",
    "3. select all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_df = (\n",
    "    fire_df\n",
    "    .where(\"delay > 5 AND delay is not null\")\n",
    "    .select(\"*\")\n",
    ")\n",
    "\n",
    "display(q3_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d52491",
   "metadata": {},
   "source": [
    "#### Q4. What were the most common call types?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649847c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.createOrReplaceTempView(\"fire_service_calls_view\")\n",
    "\n",
    "q4_sql_df  = spark.sql(\n",
    "    \"\"\"\n",
    "    select calltype , count(calltype) as count \n",
    "    from fire_service_calls_view\n",
    "    where calltype is not null \n",
    "    group by calltype \n",
    "    order by count desc \n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "display(q4_sql_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da2738",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. filter the rows where calltype is not null \n",
    "2. select calltype col \n",
    "3. group them by calltype \n",
    "4. count the grouped dataframe \n",
    "5. sort them by count in desc order \n",
    "6. show them results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6853c2",
   "metadata": {},
   "source": [
    "## GroupBy and Count in DataFrame\n",
    "\n",
    "In Spark DataFrames, the `groupBy` and `count` methods are used for grouping and aggregating data.\n",
    "\n",
    "- **`count` as an Action**: When you call `count` directly on a DataFrame, it is an action that triggers the execution of the Spark job and returns the number of rows in the DataFrame.\n",
    "\n",
    "    python\n",
    "    row_count = df.count()\n",
    "    \n",
    "\n",
    "- **`groupBy` and `count` as a Transformation**: When you call `groupBy` on a DataFrame, it returns a `GroupedData` object. You can then call `count` on this `GroupedData` object to get the count of rows for each group. This `count` is a transformation and does not trigger execution until an action is called.\n",
    "\n",
    "    python\n",
    "    grouped_df = df.groupBy(\"column_name\").count()\n",
    "    \n",
    "\n",
    "In summary:\n",
    "- `count` before `groupBy` is an action and returns the total row count.\n",
    "- `count` after `groupBy` is a transformation on the `GroupedData` object and returns a new DataFrame with the count of rows for each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_df = (\n",
    "    fire_df\n",
    "    .select(\"CallType\") # crates new transformed dataframe\n",
    "    .where(\"CallType is not null\") # creates new transformed dataframe \n",
    "    .groupby(\"CallType\") # creates relationalgroupeddataset object \n",
    "    .count() # its transformation not action # creates new transformed dataframe using relationalgroupeddataset object \n",
    "    .orderBy(\"count\",ascending=False) # creates new transformed dataframe \n",
    "    .show() # returns the newest dataframe \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b0425",
   "metadata": {},
   "source": [
    "#### Q5. What zip codes accounted for most common calls ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.createOrReplaceTempView(\"fire_calls_service_view\")\n",
    "\n",
    "q5_sql_df = spark.sql(\n",
    "    \"\"\"\n",
    "    select zipcode , count(zipcode) as count \n",
    "    from fire_calls_service_view\n",
    "    where zipcode is not null \n",
    "    group by zipcode \n",
    "    order by count desc \n",
    "    \"\"\"\n",
    ")\n",
    "display(q5_sql_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fda1a",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. select zipcode col \n",
    "2. filter teh zipcode col where its not null \n",
    "3. group it by zipcode \n",
    "4. count the grouped zipcodes \n",
    "5. order them by count desc \n",
    "6. show them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_df = (\n",
    "    fire_df\n",
    "    .select(\"Zipcode\")\n",
    "    .where(\"Zipcode is not null\")\n",
    "    .groupby(\"Zipcode\")\n",
    "    .count()\n",
    "    .orderBy(\"count\",ascending=False)\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc1b5d",
   "metadata": {},
   "source": [
    "#### Q6. What San Francisco neigbourhoods are in the zip codes 94102 and 94103 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339e54d",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. select Zipcode and Neigbourhoods col \n",
    "2. filter those col where there are no null values \n",
    "3. and filter the dataframe where city = San Francisco \n",
    "4. and again filter the dataframe where zipcode is either 94102 or 94103\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q6_df = (\n",
    "  fire_df\n",
    "  .select(\"Zipcode\",\"Neighborhood\")\n",
    "  .where(\"Zipcode is not null and Neighborhood is not null\")\n",
    "  .where(\"city= 'San Francisco'\")\n",
    "  .where(\"Zipcode = 94102 or Zipcode = 94103\")\n",
    "  .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01c3b3",
   "metadata": {},
   "source": [
    "#### Q7. What was the sum of all call alarms, average , min and max of the call response time ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc37f19",
   "metadata": {},
   "source": [
    "## Aggregations with .agg() in DataFrames\n",
    "\n",
    "In PySpark, the `.agg()` method is used to perform aggregate operations on DataFrames. This method allows you to compute summary statistics on your data, such as sums, averages, minimums, and maximums, similar to SQL's aggregate functions.\n",
    "\n",
    "### How `.agg()` Works\n",
    "\n",
    "- **Aggregating**: You use the `.agg()` method to specify the aggregation functions you want to apply to the data.\n",
    "\n",
    "### Common Aggregation Functions\n",
    "\n",
    "You can import common aggregation functions from `pyspark.sql.functions`, such as:\n",
    "- `sum()`: Computes the sum of values.\n",
    "- `avg()`: Computes the average of values.\n",
    "- `min()`: Finds the minimum value.\n",
    "- `max()`: Finds the maximum value.\n",
    "\n",
    "### Example\n",
    "\n",
    "Here is an example that shows how to use `.agg()` to compute the sum, average, minimum, and maximum of specific columns:\n",
    "\n",
    "python\n",
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "\n",
    "df_aggregated = df.agg(\n",
    "    sum(\"numalarms\").alias(\"total_alarms\"),\n",
    "    avg(\"delay\").alias(\"average_delay\"),\n",
    "    min(\"delay\").alias(\"min_delay\"),\n",
    "    max(\"delay\").alias(\"max_delay\")\n",
    ")\n",
    "\n",
    "display(df_aggregated)\n",
    "\n",
    "\n",
    "In this example:\n",
    "- `.agg(sum(\"numalarms\").alias(\"total_alarms\"), avg(\"delay\").alias(\"average_delay\"), min(\"delay\").alias(\"min_delay\"), max(\"delay\").alias(\"max_delay\"))` computes the sum, average, minimum, and maximum for the specified columns and renames the resulting columns accordingly.\n",
    "\n",
    "Using `.agg()` is a powerful way to perform complex aggregations and derive meaningful insights from your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a73a99",
   "metadata": {},
   "source": [
    "- `sum`, `avg`, `min`, `max` produce Column expressions.\n",
    "- `.agg()` consumes those expressions and returns a new DataFrame.\n",
    "- Only the `.agg()` call is what creates a new DataFrame, not the individual aggregate functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "q7_df = (\n",
    "  fire_df\n",
    "  .agg( \n",
    "    sum(\"numalarms\").alias(\"total_alarms\"), # col expression/object\n",
    "    avg(\"delay\").alias(\"average_delay\"), # col expression/object\n",
    "    min(\"delay\").alias(\"min_delay\"), # col expression/object\n",
    "    max(\"delay\").alias(\"max_delay\") # col expression/object\n",
    "  ) # agg creates a new transformed dataframe \n",
    ")\n",
    "display(q7_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb5de7b",
   "metadata": {},
   "source": [
    "#### Q8. How many distinct years of data is in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780523b",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. from calldata col extract year and select it \n",
    "2. filter the col with distinct years \n",
    "3. order it by distinct_years in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51abb8e",
   "metadata": {},
   "source": [
    "## Extracting Year from a Column in a DataFrame\n",
    "\n",
    "To extract the year from a date column in a DataFrame, you can use the `year` function from `pyspark.sql.functions`. This function expects a column object, which you can create using the `col` function. Here is how you can do it:\n",
    "\n",
    "1. **Select and Extract Year**: Use the `select` method to choose the column you want to extract the year from. Apply the `year` function to this column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e28d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q8_df = (\n",
    "    fire_df\n",
    "    .select(year(col(\"CallDate\")).alias(\"distinct_years\"))\n",
    "    .distinct()\n",
    "    .orderBy(\"distinct_years\")\n",
    ")\n",
    "display(q8_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e118e",
   "metadata": {},
   "source": [
    "#### Q9. What week of the year in 2018 had the most fire calls ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d932a3",
   "metadata": {},
   "source": [
    "dataframe logic \n",
    "1. create a new col week_year to convert call date in weekofyear \n",
    "2. filter the rows where calldate year = 2018\n",
    "3. group by week_year \n",
    "4. count the grouped by week_year \n",
    "5. order by count in desc order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d02ae",
   "metadata": {},
   "source": [
    "### weekofyear Function\n",
    "\n",
    "The `weekofyear` function in PySpark is used to extract the week number from a given date. This function returns an integer representing the week of the year for the specified date.\n",
    "\n",
    "#### Syntax\n",
    "python\n",
    "weekofyear(expr)\n",
    "\n",
    "\n",
    "#### Arguments\n",
    "- `expr`: A DATE expression. This can be a column containing date values.\n",
    "\n",
    "#### Returns\n",
    "- An INTEGER representing the week of the year.\n",
    "\n",
    "#### Example\n",
    "python\n",
    "from pyspark.sql.functions import weekofyear, col\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',), ('2024-10-31',)], ['dt'])\n",
    "df.select(\"*\", weekofyear(col(\"dt\")).alias(\"week_of_year\")).show()\n",
    "\n",
    "\n",
    "In this example, the `weekofyear` function extracts the week number from the date column `dt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, weekofyear, year, count\n",
    "\n",
    "q9_df = (\n",
    "  fire_df\n",
    "  .withColumn(\"week_year\", weekofyear(col(\"CallDate\")))\n",
    "  .where(year(col(\"CallDate\")) == 2018)\n",
    "  .groupBy(\"week_year\")\n",
    "  .agg(count(\"*\").alias(\"total_calls\"))\n",
    "  .orderBy(col(\"total_calls\").desc())\n",
    ")\n",
    "display(q9_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}