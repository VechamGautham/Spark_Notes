{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c47e798",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "- [Spark RDD (Resilient Distributed Dataset)](#spark-rdd-resilient-distributed-dataset)\n",
    "- [How to use Spark SQL](#how-to-use-spark-sql)\n",
    "- [Catalyst Optimzer in Spark](#catalyst-optimizer-in-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33413519",
   "metadata": {},
   "source": [
    "# Spark RDD (Resilient Distributed Dataset)\n",
    "\n",
    "## 1. What is an RDD?\n",
    "- **RDD** = Resilient Distributed Dataset.  \n",
    "- The **core abstraction** in Spark (low-level API).  \n",
    "- Represents a **distributed collection of objects**:\n",
    "  - Partitioned across cluster nodes.\n",
    "  - Fault-tolerant ‚Üí if a partition is lost, Spark can **recompute** it from lineage.\n",
    "- Think of it as a **big list/array split across machines**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What does it do?\n",
    "- Lets you perform **parallel operations** on data:\n",
    "  - Transformations: `map()`, `filter()`, `flatMap()`, `reduceByKey()`.\n",
    "  - Actions: `collect()`, `count()`, `saveAsTextFile()`.\n",
    "- Works with **raw objects** (no schema, no columns).  \n",
    "- You can build custom logic with Python/Scala/Java functions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why it‚Äôs not used much today\n",
    "- **No schema awareness**: Spark doesn‚Äôt know about columns/types.\n",
    "- **Hard to optimize**: Catalyst optimizer (query planner) cannot peek into user functions.\n",
    "- **Developer overhead**: You must manually parse, structure, and manage data types.\n",
    "- **DataFrames/SQL are better**:\n",
    "  - Provide schema (rows + columns).\n",
    "  - Optimized by Catalyst engine (faster execution).\n",
    "  - Easier to write and maintain with SQL-like operations.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- RDD = Spark‚Äôs original, low-level API for distributed collections.  \n",
    "- Still used internally by Spark, but **rarely written by developers** today.  \n",
    "- Modern Spark code ‚Üí **DataFrames/SQL API** (faster, easier, optimized)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7df71",
   "metadata": {},
   "source": [
    "# How to Use Spark SQL\n",
    "\n",
    "Below is a simple pattern you can follow. Your mental model is right:\n",
    "1) **Create a SparkSession** ‚Üí 2) **Load data into a DataFrame** (CSV/Parquet/etc.) ‚Üí  \n",
    "3) **Register a temp view** ‚Üí 4) **Write SQL with `spark.sql(...)`** ‚Üí 5) **Action** (e.g., `show()`/`write`).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå One-file Example (read ‚Üí temp view ‚Üí SQL)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- 1) Create SparkSession ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"SparkSQLDemo\")\n",
    "        .config(\"spark.ui.port\", \"4050\")  # optional: keep UI off default 4040\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "\n",
    "# --- 2) Load data into a DataFrame ---\n",
    "# Option A: CSV (use inferSchema if columns are typed; header if first line has names)\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"/path/to/your/survey.csv\")\n",
    ")\n",
    "\n",
    "# # Option B: Parquet (schema is embedded; faster + columnar)\n",
    "# df = spark.read.parquet(\"/path/to/your/survey.parquet\")\n",
    "\n",
    "# --- 3) Create or replace a temp view (session-scoped) ---\n",
    "df.createOrReplaceTempView(\"survey_tbl\")\n",
    "# Tip: use Global Temp View for cross-session visibility:\n",
    "# df.createOrReplaceGlobalTempView(\"survey_tbl\")  # then query as SELECT * FROM global_temp.survey_tbl\n",
    "\n",
    "# --- 4) Write SQL using spark.sql(...) ---\n",
    "# Use COUNT(*) or COUNT(1); add spaces and proper operators in predicates.\n",
    "# Example: count rows per Country where Age < 40\n",
    "sql_query = \"\"\"\n",
    "SELECT Country, COUNT(*) AS cnt\n",
    "FROM survey_tbl\n",
    "WHERE Age < 40\n",
    "GROUP BY Country\n",
    "ORDER BY cnt DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "# --- 5) Trigger an action to execute the plan ---\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# (Optional) Save results\n",
    "# result_df.write.mode(\"overwrite\").parquet(\"/tmp/survey_by_country_lt40\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "<pre>\n",
    "üß≠ Notes & Fixes (common gotchas)\n",
    "\t‚Ä¢\tCOUNT syntax: prefer COUNT(*) or COUNT(1) rather than count() with empty parens.\n",
    "\t‚Ä¢\tTemp View scope:\n",
    "\t‚Ä¢\tcreateOrReplaceTempView(\"name\") ‚Üí visible only in this SparkSession.\n",
    "\t‚Ä¢\tcreateOrReplaceGlobalTempView(\"name\") ‚Üí query as global_temp.name (across sessions).\n",
    "\t‚Ä¢\tSchema:\n",
    "\t‚Ä¢\tCSV often needs .option(\"inferSchema\",\"true\") (costs a small scan).\n",
    "\t‚Ä¢\tParquet/ORC already carry schema (faster + better for analytics).\n",
    "\t‚Ä¢\tSQL vs DataFrame:\n",
    "\t‚Ä¢\tAnything you do in SQL has an equivalent in the DataFrame API and vice versa (result_df.explain(\"formatted\") to inspect the plan).\n",
    "\t‚Ä¢\tExecution:\n",
    "\t‚Ä¢\tTransformations are lazy; calling show()/write/collect triggers the job.\n",
    "\t‚Ä¢\tCheck Spark UI (printed URL) for Jobs ‚Üí Stages ‚Üí Tasks and shuffles.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "‚úÖ TL;DR\n",
    "\t‚Ä¢\tLoad to DataFrame ‚Üí createOrReplaceTempView(\"table\") ‚Üí run SQL with spark.sql(\"...\") ‚Üí action.\n",
    "\t‚Ä¢\tUse COUNT(*), ensure predicates are valid (e.g., WHERE Age < 40), prefer Parquet for speed in production.\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f5e59",
   "metadata": {},
   "source": [
    "# Catalyst Optimizer in Spark\n",
    "\n",
    "![Catalyst Optimizer](./images/catalyst_optimiser.png)\n",
    "\n",
    "## üîé What is Catalyst?\n",
    "- Catalyst is the **query optimization framework** inside the **Spark SQL Engine**.  \n",
    "- It powers **DataFrame** and **SQL APIs**.  \n",
    "- Its job: **analyze, optimize, and generate efficient execution plans** for your code.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Catalyst Workflow (Four Phases)\n",
    "\n",
    "### 1. **Analysis**\n",
    "- Spark reads your SQL/DataFrame query and builds an **Abstract Syntax Tree (AST)**.  \n",
    "- Resolves:\n",
    "  - Table/view names\n",
    "  - Column names\n",
    "  - SQL functions  \n",
    "- Errors like ‚Äúcolumn not found‚Äù appear at this stage.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Logical Optimization**\n",
    "- Catalyst applies **rule-based** and **cost-based** optimizations:\n",
    "  - **Predicate Pushdown** ‚Üí push filters closer to the data source.  \n",
    "  - **Projection Pruning** ‚Üí drop unused columns early.  \n",
    "  - **Constant Folding** ‚Üí pre-compute constants (e.g., `2+2 ‚Üí 4`).  \n",
    "  - **Boolean Simplification** ‚Üí simplify conditions (`x AND true ‚Üí x`).  \n",
    "- Produces multiple logical plans and picks the best one.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Physical Planning**\n",
    "- The chosen logical plan is mapped to a **physical plan**:  \n",
    "  - Concrete execution strategy (joins, shuffles, scans).  \n",
    "  - Uses **RDD operations** under the hood.  \n",
    "- Spark may consider multiple strategies (e.g., broadcast join vs shuffle join) and choose the cheapest.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Code Generation (Whole-Stage Codegen)**\n",
    "- Introduced in **Project Tungsten (Spark 2.0)**.  \n",
    "- Compiles the physical plan into **optimized Java bytecode**.  \n",
    "- Benefits:\n",
    "  - Fewer virtual function calls\n",
    "  - Better CPU utilization\n",
    "  - Faster execution on each executor\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why it matters\n",
    "- As a developer, you **don‚Äôt need to hand-optimize** queries.  \n",
    "- Just use **DataFrames/SQL**, and Spark‚Äôs **Catalyst Optimizer** automatically:\n",
    "  - Builds efficient plans\n",
    "  - Reduces shuffles\n",
    "  - Improves speed\n",
    "\n",
    "---\n",
    "\n",
    "## üîë TL;DR\n",
    "- Catalyst = Spark‚Äôs brain for **query optimization**.  \n",
    "- Stages: **Analysis ‚Üí Logical Optimization ‚Üí Physical Planning ‚Üí Code Generation**.  \n",
    "- Use DataFrames/SQL to benefit from it automatically."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
