{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89f29b1",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "- [pyspark .join()](#pyspark-dataframe-join--quick-guide)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facea9ec",
   "metadata": {},
   "source": [
    "# PySpark DataFrame `.join()` — quick guide\n",
    "\n",
    "**Purpose:** Combine two DataFrames row-wise based on matching key(s).\n",
    "\n",
    "**Signature**\n",
    "    dataframe1.join(dataframe2, on=None, how=None)\n",
    "\n",
    "**Arguments**\n",
    "- `dataframe2` → The right/second DataFrame.\n",
    "- `on` → Join condition. Any one of:\n",
    "  - **String** (shared column name)\n",
    "        df1.join(df2, \"id\", \"inner\")\n",
    "  - **List of strings** (multi-column key)\n",
    "        df1.join(df2, [\"id\", \"date\"], \"left\")\n",
    "  - **Column expression** (boolean)\n",
    "        from pyspark.sql.functions import col\n",
    "        df1.join(df2, col(\"df1_id\") == col(\"df2_id\"), \"inner\")\n",
    "    • If you prefer SQL syntax, wrap it with `expr(...)` to get a Column:\n",
    "        from pyspark.sql.functions import expr\n",
    "        df1.join(df2, expr(\"df1.id = df2.emp_id\"), \"inner\")\n",
    "- `how` → Join type (default `\"inner\"`). Options:\n",
    "  - `\"inner\"` — only matches\n",
    "  - `\"left\"` / `\"left_outer\"` — all left + matches from right\n",
    "  - `\"right\"` / `\"right_outer\"` — all right + matches from left\n",
    "  - `\"outer\"` / `\"full\"` / `\"full_outer\"` — all rows from both\n",
    "  - `\"left_semi\"` — rows from left **that have a match** (returns only left columns)\n",
    "  - `\"left_anti\"` — rows from left **that do not have a match**\n",
    "  - `\"cross\"` — Cartesian product (no `on`)\n",
    "\n",
    "**Examples**\n",
    "    # 1) Inner join on same column name\n",
    "    df1.join(df2, \"id\", \"inner\")\n",
    "\n",
    "    # 2) Left join on multiple keys\n",
    "    df1.join(df2, [\"id\", \"dt\"], \"left\")\n",
    "\n",
    "    # 3) Join with boolean expression\n",
    "    df1.join(df2, df1[\"id\"] == df2[\"emp_id\"], \"outer\")\n",
    "\n",
    "    # 4) Using expr() to write SQL-like condition\n",
    "    from pyspark.sql.functions import expr\n",
    "    df1.join(df2, expr(\"df1.id = df2.emp_id AND df1.dt = df2.dt\"), \"left_outer\")\n",
    "\n",
    "    # 5) Semi / Anti joins (filtering left by existence in right)\n",
    "    df_left_semi = orders.join(customers, \"cust_id\", \"left_semi\")\n",
    "    df_left_anti = orders.join(customers, \"cust_id\", \"left_anti\")\n",
    "\n",
    "**Notes & tips**\n",
    "- When `on` is a **string/list**, Spark deduplicates the join key(s) (they appear once).  \n",
    "  When using a **Column expression**, both key columns remain—`select()`/`drop()` or alias as needed.\n",
    "- Prefer joining on **equi-keys** (e.g., `==`) for performance and broadcast when one side is small:\n",
    "      from pyspark.sql.functions import broadcast\n",
    "      big.join(broadcast(small), \"id\", \"inner\")\n",
    "- Avoid column name collisions for non-key columns by renaming before join or selecting explicit columns after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be0bb8",
   "metadata": {},
   "source": [
    "# Demystifying Spark **Joins**, **Exchanges**, and the **Shuffle** (with the `data_1` / `data_2` demo)\n",
    "\n",
    "> Big picture: most “slow joins” in Spark are really “slow **shuffles**.” A join over two DataFrames runs two *parent* (map) stages that **write** shuffle buckets, followed by one *child* (reduce) stage that **reads** those buckets and executes the join (typically **Sort-Merge Join**). The **Exchange** nodes you see in the plan are the points where data is **repartitioned and moved**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Demo Setup (the exact scenario)\n",
    "- Two folders: **`data_1/`** and **`data_2/`**\n",
    "- Each folder contains **3 JSON files** → Spark creates **3 input partitions** per DataFrame\n",
    "- Local session: **`master(\"local[3]\")`** → 1 executor process with **3 task slots** (max 3 tasks at once)\n",
    "- Shuffle partitions: **`spark.sql.shuffle.partitions = 3`** → the post-shuffle (reduce) side will have **3 partitions**\n",
    "- Operation: **inner join on `id`** + a terminal **action** (`count()`, `show()`, or your `foreach`)\n",
    "\n",
    "This configuration ensures:\n",
    "- The scan of `data_1` uses **3 tasks**, scan of `data_2` uses **3 tasks**\n",
    "- The reduce side of the join runs **3 tasks** (partitions 0, 1, 2)\n",
    "- Because of `local[3]`, Spark can execute **at most 3 tasks concurrently** across all stages\n",
    "\n",
    "---\n",
    "\n",
    "## 2) What an **Exchange** is (and why it matters)\n",
    "In Spark’s physical plan, an **Exchange** marks a boundary where data is **redistributed** (repartitioned) across the cluster (or across threads in local mode).\n",
    "\n",
    "- **Map-side Exchange (Shuffle *Write*)**  \n",
    "  For each input DataFrame, Spark reads rows from its current partitions and decides the destination reduce partition via a partitioner (commonly **`hash(joinKey) % numShufflePartitions`**). It writes one **shuffle bucket** per destination partition (spillable to disk).\n",
    "- **Reduce-side Exchange (Shuffle *Read*)**  \n",
    "  The reduce tasks **fetch** bucket *k* from **all map tasks of both inputs**, merge them, and materialize reduce partition *k*. Now “all rows for a given key” are co-located, enabling the actual join.\n",
    "\n",
    "Mental model: **Scatter → Gather**  \n",
    "Map stages **scatter** rows into N buckets; reduce stages **gather** bucket *k* from everywhere and process it.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) The three stages of an inner join (what you see in the UI)\n",
    "With `data_1` (3 partitions) joined to `data_2` (3 partitions) and `spark.sql.shuffle.partitions = 3`, the join job breaks into **three stages**:\n",
    "\n",
    "1) **Stage A — Map side for `data_1` (Shuffle Write)**  \n",
    "   - Tasks: **3** (one per input partition)  \n",
    "   - For each row: compute `target = hash(id) % 3` and append to shuffle bucket `target`  \n",
    "   - Output: 3 buckets per task → later read by the reduce stage\n",
    "\n",
    "2) **Stage B — Map side for `data_2` (Shuffle Write)**  \n",
    "   - Tasks: **3** (one per input partition)  \n",
    "   - Same process; produces buckets keyed by the same partitioner\n",
    "\n",
    "3) **Stage C — Reduce side (Shuffle Read + Sort-Merge Join)**  \n",
    "   - Tasks: **3** (for reduce partitions **0, 1, 2**)  \n",
    "   - Each task **fetches** bucket *k* from **all 3 map tasks of `data_1`** and **all 3 map tasks of `data_2`**, merges, sorts by `id` if needed, and performs the **join** to produce the final partition *k*\n",
    "\n",
    "Totals you’ll notice:\n",
    "- **9 tasks** in the join job: **3 + 3 + 3**\n",
    "- Because you’re on **`local[3]`**, they run in **waves of 3** (never more than 3 concurrent tasks)\n",
    "\n",
    "---\n",
    "\n",
    "## 4) “Parallel” with only 3 cores? (concurrency vs. eligibility)\n",
    "Both parent stages (A for `data_1`, B for `data_2`) are **eligible at the same time**, but you only have **3 slots**. Spark interleaves them to keep slots busy:\n",
    "\n",
    "Slots (3 total) timeline illustration:\n",
    "- Slot 1: A1 → A3 → C1\n",
    "- Slot 2: A2 → B2 → C2\n",
    "- Slot 3: B1 → B3 → C3\n",
    "\n",
    "Key point: at any instant you see **≤ 3** running tasks, but those 3 can come from **both** map stages. Once all parent tasks finish, the reduce stage (C) runs its 3 tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Why shuffle dominates cost\n",
    "- **Network I/O**: every reduce task pulls its bucket from **all** map tasks of both inputs (many small remote reads)\n",
    "- **Disk I/O**: map tasks **spill** shuffle files; reduce tasks **merge** them\n",
    "- **Serialization / deserialization** and **sorting**\n",
    "- **Skew**: a hot key can make one reduce partition dramatically slower than others\n",
    "\n",
    "Rule of thumb: **“slow join” = “expensive shuffle”** far more often than not.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Reading vs. Shuffling vs. Joining — how to *visualize* it\n",
    "1) **Input scans (Jobs 0 & 1 in your UI)**  \n",
    "   - Each DataFrame read appears as a simple job with **1 stage / 3 tasks** (schema inference for JSON can add a tiny job, too)\n",
    "2) **Join job (Job 2)**  \n",
    "   - **Stage A (3 tasks)**: map/exchange for `data_1` (shuffle write)  \n",
    "   - **Stage B (3 tasks)**: map/exchange for `data_2` (shuffle write)  \n",
    "   - **Stage C (3 tasks)**: reduce/exchange + sort-merge join (shuffle read)  \n",
    "   - UI often shows arrows between A/B and C (the **shuffle** edges)\n",
    "\n",
    "If you set `spark.sql.adaptive.enabled=true` (default in newer Spark), the UI may show **skipped** or **coalesced** stages/partitions as AQE optimizes at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Broadcast Join (the alternative that skips the big shuffle)\n",
    "If one side is **small enough** (fits under `spark.sql.autoBroadcastJoinThreshold`, e.g., ~10–20 MB by default), Spark can **broadcast** that DataFrame to all executors. Then the big side **doesn’t shuffle**:\n",
    "- Plan shows a **BroadcastExchange** for the small side\n",
    "- Join becomes **BroadcastHashJoin**\n",
    "- You can force it with a hint: `df_small.hint(\"broadcast\")`\n",
    "\n",
    "Broadcasting avoids the “scatter–gather” shuffle for the big side and is often the fastest option when applicable.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Practical tuning knobs (for shuffle joins)\n",
    "- **`spark.sql.shuffle.partitions`**:  \n",
    "  Too **high** → many tiny tasks (scheduler overhead). Too **low** → fat partitions (skew/stragglers). A starting heuristic: **2–4× total cores** available to the job, then adjust with metrics.\n",
    "- **Adaptive Query Execution (AQE)**:  \n",
    "  Keep **on** unless you need deterministic, tutorial-style plans. It can **coalesce** post-shuffle partitions and **handle skew**.\n",
    "- **Skew mitigation**:  \n",
    "  Salt hot keys (add a random suffix before join), or use AQE’s skew join handling when available.\n",
    "- **File sizing**:  \n",
    "  Avoid thousands of tiny files; aim for sensible partition/file sizes to reduce overhead.\n",
    "- **Filter early, select only needed columns** to reduce shuffle volume.\n",
    "- **Prefer Broadcast Join** whenever one side is small.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Quick mental checklist (cheat sheet)\n",
    "- Partition → **task**; **1 core ≈ 1 concurrent task** slot\n",
    "- **Exchange** = repartition + data movement\n",
    "- **Shuffle write** (parents) → **Shuffle read** (children)\n",
    "- Inner join with two inputs ⇒ usually **two map stages + one reduce stage**\n",
    "- Total tasks you saw: **3 (map A) + 3 (map B) + 3 (reduce C) = 9**\n",
    "- On `local[3]`, tasks run in **waves of 3**\n",
    "- Optimize joins by **reducing shuffled bytes**, **handling skew**, and **choosing broadcast** when possible\n",
    "\n",
    "---\n",
    "\n",
    "## 10) One-screen summary diagram\n",
    "<pre>\n",
    "Inputs (3 partitions each)\n",
    "  data_1: P1, P2, P3          data_2: Q1, Q2, Q3\n",
    "         │   │   │                    │   │   │\n",
    "         ├───┴───┤  Map/Shuffle-Write ├───┴───┤\n",
    "         │ hash(id) % 3               │ hash(id) % 3\n",
    "         ▼   ▼   ▼                    ▼   ▼   ▼\n",
    "       buckets 0/1/2                buckets 0/1/2\n",
    "             \\                           /\n",
    "              \\_________________________/\n",
    "                       ▼  Shuffle-Read\n",
    "                Reduce partitions 0,1,2\n",
    "                  (Sort-Merge Join)\n",
    "                       ▼\n",
    "                 Joined Output\n",
    "                 P0, P1, P2\n",
    "<pre>\n",
    "\n",
    "That’s the full story behind your Spark UI showing **3 stages** and **9/9 tasks** for the join job, and why **Exchange** and **shuffle** are the heart of join performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501eef1",
   "metadata": {},
   "source": [
    "# Solving Spark Join Performance Problems — A Simple Playbook\n",
    "\n",
    "Joins are often the #1 reason Spark jobs slow down, because they trigger **shuffles** (lots of network I/O and disk I/O). Here’s a clear, copy-paste guide to make your joins faster and safer.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Choose the Right Join Strategy\n",
    "- **Broadcast Join (large ↔ small):** If one table is small enough to fit in executor memory, broadcast it so the large table doesn’t shuffle.\n",
    "  - When to use: small side is tens/hundreds of MBs (rough guide; depends on your cluster).\n",
    "  - How: force with a broadcast hint if Spark doesn’t auto-choose it.\n",
    "- **Shuffle (Sort-Merge) Join (large ↔ large):** Unavoidable when both sides are big. Focus on shrinking data and balancing work (see below).\n",
    "\n",
    "Tip: Always check the physical plan (`explain`) for **BroadcastHashJoin** vs **SortMergeJoin**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Shrink Data *Before* the Join\n",
    "- **Filter early:** Remove rows that cannot match (e.g., keep only “US” rows if the other table is US-only).\n",
    "- **Project early:** Select only columns you actually need.\n",
    "- **Aggregate early:** Pre-aggregate facts to the grain required for the join/output.\n",
    "- **Deduplicate:** Drop duplicates on join keys if appropriate to avoid cartesian explosions.\n",
    "\n",
    "Small inputs → smaller shuffle → faster join.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Tune Parallelism Without Overhead\n",
    "- **Shuffle partitions (`spark.sql.shuffle.partitions`):** Controls reduce-side parallelism for shuffle joins.\n",
    "  - Start near **2–4× total task slots** (executors × cores) if AQE is **off**.\n",
    "  - If **AQE is ON** (recommended), it can **coalesce** partitions: set this higher and let AQE merge small ones.\n",
    "- **Target partition size:** Aim for ~**128–256 MB** per task to avoid tiny tasks (overhead) and giant tasks (spill/stragglers).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Handle Key Skew (Hot Keys) — The Silent Job Killer\n",
    "When a few keys have most of the rows, their reduce tasks run much longer.\n",
    "\n",
    "Fixes:\n",
    "- **AQE Skew Join:** Enable adaptive query execution so Spark **splits** oversized shuffle partitions at runtime.\n",
    "- **Salting hot keys:** For the biggest keys, add a small **salt** bucket (e.g., 0..7) and join on `(key, salt)` so multiple tasks work the same key in parallel. Aggregate back after the join if needed.\n",
    "- **Pre-aggregate by key:** Reduce rows per hot key *before* joining.\n",
    "- **Bucketing:** If you repeatedly join on the same key, bucket both tables on that key (and optionally sort). This can avoid or reduce shuffle on recurring pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Reduce Network I/O (Move Fewer Bytes)\n",
    "- **Filter/Project/Aggregate early** (repeating because it’s that important).\n",
    "- **Columnar formats** (Parquet/ORC) with predicate pushdown to trim reads.\n",
    "- **Partition pruning** on the read side (e.g., date/state folders) so you don’t even load irrelevant data.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Practical Patterns (minimal examples)\n",
    "\n",
    "Broadcast the small side:\n",
    "    # assume df_big (large), df_small (fits in memory)\n",
    "    # from pyspark.sql.functions import broadcast\n",
    "    joined = df_big.join(broadcast(df_small), \"id\", \"inner\")\n",
    "\n",
    "Pre-aggregate before join:\n",
    "    pre = df_big.groupBy(\"id\").agg(F.sum(\"amount\").alias(\"amt\"))\n",
    "    joined = pre.join(df_small, \"id\", \"left\")\n",
    "\n",
    "Salt a hot key (fan-out only for the hottest keys):\n",
    "    hot = [\"K1\",\"K2\"]               # discovered from key counts\n",
    "    S = 8                           # split factor for hot keys\n",
    "    big_salted = df_big.withColumn(\n",
    "        \"salt\",\n",
    "        F.when(F.col(\"key\").isin(hot), F.abs(F.hash(F.monotonically_increasing_id())) % S).otherwise(F.lit(0))\n",
    "    )\n",
    "    small_expanded = df_small.withColumn(\n",
    "        \"salt\",\n",
    "        F.when(F.col(\"key\").isin(hot), F.explode(F.array([F.lit(i) for i in range(S)]))).otherwise(F.lit(0))\n",
    "    )\n",
    "    joined = big_salted.join(small_expanded, [\"key\",\"salt\"], \"inner\")\n",
    "    result = joined.groupBy(\"key\").agg(F.sum(\"metric\").alias(\"metric_sum\"))  # optional “unsalt”\n",
    "\n",
    "Enable AQE (recommended defaults):\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Quick Checklist (use every time)\n",
    "- Did I **filter**, **project**, and **aggregate** before joining?\n",
    "- Is **broadcast** viable (and forced if needed)?\n",
    "- Is **AQE** on, with skew join enabled?\n",
    "- Are **shuffle partitions** tuned (or high with AQE to coalesce)?\n",
    "- Any **skewed keys**? (Check Spark UI: long tasks, big shuffle-read size)\n",
    "- Do I need **salting** or **bucketing** for recurring joins?\n",
    "- Are partition/file sizes sane (avoid too many tiny files)?\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Simple Rules of Thumb\n",
    "- Prefer **broadcast** when one side is small → avoids big shuffle.\n",
    "- For shuffle joins, keep partitions around **128–256 MB**.\n",
    "- If you have **more cores than distinct keys**, you’ll underutilize the cluster unless you **increase effective cardinality** (salting or extra bucketing dimension).\n",
    "- **Measure**: Spark UI “Shuffle Read/Write” and task timelines will tell you where the pain is.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:** Make the data smaller first, pick the right join (broadcast when possible), give Spark enough (but not absurd) parallelism, and neutralize skew with AQE or salting. Do those four, and most join performance problems disappear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
