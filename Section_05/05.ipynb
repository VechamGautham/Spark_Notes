{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afbb50f",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "-[Spark Dataframe Partitions](#spark-dataframe-partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45fab",
   "metadata": {},
   "source": [
    "# Spark DataFrame Partitions\n",
    "\n",
    "## 1. Data Stored in Distributed Systems\n",
    "- In real life, files (CSV, JSON, etc.) are stored in **distributed storage** like HDFS or Amazon S3.  \n",
    "- The file is **split into partitions** and spread across nodes.  \n",
    "  - Example: 100 partitions across 10 nodes.  \n",
    "- This splitting makes **parallel reading** possible.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logical DataFrame\n",
    "- When you call `spark.read.csv(\"file.csv\")`, Spark creates a **logical DataFrame**:\n",
    "  - Stores **metadata**: schema, partition info, and how to read them.\n",
    "  - **No data is loaded yet** (lazy evaluation).  \n",
    "- Think of it like a **recipe**: instructions exist, but the meal (data) isn‚Äôt cooked until you trigger an action.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Role of the Driver (SparkSession)\n",
    "- The driver (your `spark` session) is the **brain**:\n",
    "  - Contacts cluster manager + storage.\n",
    "  - Collects info about partitions.\n",
    "  - Creates the **plan** to process them.\n",
    "- Still, the driver does not load data itself.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Executors (the workers)\n",
    "- Executors are **JVM processes** launched by the cluster manager.  \n",
    "- They do the **real work**:\n",
    "  - Load their assigned partitions into memory.\n",
    "  - Run tasks (filter, join, aggregate, etc.).  \n",
    "- Each executor has multiple **cores**, and each core processes **one partition at a time**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Answer to the Doubt ‚úÖ\n",
    "- If you have **5 executors** and **5 cores per executor**:  \n",
    "  - Total = `5 √ó 5 = 25 cores`.  \n",
    "- That means **25 partitions can be processed in parallel**.  \n",
    "- ‚ö†Ô∏è Notes:\n",
    "  - If partitions < 25, some cores stay idle.  \n",
    "  - If partitions > 25, Spark processes them in **waves** (25 at a time).  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Data Locality Optimization\n",
    "- Spark tries to assign each partition to an executor **close to where the data is stored**.  \n",
    "- This reduces network traffic and speeds up jobs.  \n",
    "- If not possible, Spark still works, but with some data transfer over the network.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Final Picture\n",
    "A **distributed DataFrame** is created:\n",
    "- Driver manages the plan.  \n",
    "- Executors (with multiple cores) load and process partitions in parallel.  \n",
    "- Together, they form a **scalable system** for big data.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Diagram\n",
    "![Spark DataFrame Partitions](./images/driver_execution.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
