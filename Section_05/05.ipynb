{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afbb50f",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "- [Spark Dataframe Partitions](#spark-dataframe-partitions)\n",
    "-  [Spark Transformation and Dependencies](#spark-transformations--dependencies)\n",
    "    - [Narrow Dependency](#2-narrow-dependency-transformation)\n",
    "    - [Wide Dependency](#3-wide-dependency-transformation) \n",
    "- [Spark Execution Plan](#spark-execution-plan--jobs-stages-and-tasks)\n",
    "    - [How spark code executes (jobs,stages and tasks)](#how-this-spark-code-executes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45fab",
   "metadata": {},
   "source": [
    "# Spark DataFrame Partitions\n",
    "\n",
    "## 1. Data Stored in Distributed Systems\n",
    "- In real life, files (CSV, JSON, etc.) are stored in **distributed storage** like HDFS or Amazon S3.  \n",
    "- The file is **split into partitions** and spread across nodes.  \n",
    "  - Example: 100 partitions across 10 nodes.  \n",
    "- This splitting makes **parallel reading** possible.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logical DataFrame\n",
    "- When you call `spark.read.csv(\"file.csv\")`, Spark creates a **logical DataFrame**:\n",
    "  - Stores **metadata**: schema, partition info, and how to read them.\n",
    "  - **No data is loaded yet** (lazy evaluation).  \n",
    "- Think of it like a **recipe**: instructions exist, but the meal (data) isn‚Äôt cooked until you trigger an action.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Role of the Driver (SparkSession)\n",
    "- The driver (your `spark` session) is the **brain**:\n",
    "  - Contacts cluster manager + storage.\n",
    "  - Collects info about partitions.\n",
    "  - Creates the **plan** to process them.\n",
    "- Still, the driver does not load data itself.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Executors (the workers)\n",
    "- Executors are **JVM processes** launched by the cluster manager.  \n",
    "- They do the **real work**:\n",
    "  - Load their assigned partitions into memory.\n",
    "  - Run tasks (filter, join, aggregate, etc.).  \n",
    "- Each executor has multiple **cores**, and each core processes **one partition at a time**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Answer to the Doubt ‚úÖ\n",
    "- If you have **5 executors** and **5 cores per executor**:  \n",
    "  - Total = `5 √ó 5 = 25 cores`.  \n",
    "- That means **25 partitions can be processed in parallel**.  \n",
    "- ‚ö†Ô∏è Notes:\n",
    "  - If partitions < 25, some cores stay idle.  \n",
    "  - If partitions > 25, Spark processes them in **waves** (25 at a time).  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Data Locality Optimization\n",
    "- Spark tries to assign each partition to an executor **close to where the data is stored**.  \n",
    "- This reduces network traffic and speeds up jobs.  \n",
    "- If not possible, Spark still works, but with some data transfer over the network.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Final Picture\n",
    "A **distributed DataFrame** is created:\n",
    "- Driver manages the plan.  \n",
    "- Executors (with multiple cores) load and process partitions in parallel.  \n",
    "- Together, they form a **scalable system** for big data.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Diagram\n",
    "![Spark DataFrame Partitions](./images/driver_execution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8642e",
   "metadata": {},
   "source": [
    "# Spark Transformations & Dependencies\n",
    "\n",
    "## 1. What are Transformations?\n",
    "- Spark DataFrames are **immutable**.  \n",
    "- To process data, you don‚Äôt ‚Äúmodify‚Äù a DataFrame. Instead, you apply **transformations**:\n",
    "  - Examples: `select()`, `filter()`, `groupBy()`, `orderBy()`.\n",
    "- Each transformation produces a **new DataFrame** (logically), building a chain of operations.\n",
    "- Together, transformations form a **DAG (Directed Acyclic Graph)** of operations.\n",
    "\n",
    "üìä Example:  \n",
    "![Spark Transformations](./images/transformations.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Narrow Dependency Transformation\n",
    "- A transformation is **narrow** when each partition can be processed **independently**, without needing data from other partitions.\n",
    "- Executors can process partitions locally ‚Üí results are later combined.  \n",
    "- These are usually **map-style operations**.\n",
    "\n",
    "### üîπ Examples of Narrow Dependencies:\n",
    "- `select(\"col1\", \"col2\")` ‚Üí choosing columns.  \n",
    "- `filter(df.col > 10)` / `where(df.col < 40)` ‚Üí row filtering.  \n",
    "- `withColumn(\"newCol\", df.col * 2)` ‚Üí column-level transformation.  \n",
    "- `map()` / `flatMap()` ‚Üí functional transformations.  \n",
    "- `count()` (after shuffle stage is done) ‚Üí counts within a partition.  \n",
    "\n",
    "üìä Visualization:  \n",
    "![Narrow Dependency](./images/narrow_dependency.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Wide Dependency Transformation\n",
    "- A transformation is **wide** when data from **multiple partitions must be shuffled** to produce a correct result.\n",
    "- Spark performs a **shuffle/sort exchange** ‚Üí redistributes data across partitions.  \n",
    "- These are usually **reduce-style operations**.\n",
    "\n",
    "### üîπ Examples of Wide Dependencies:\n",
    "- `groupBy(\"col\").count()` ‚Üí needs all rows of a group together.  \n",
    "- `orderBy(\"col\")` / `sort()` ‚Üí needs global ordering.  \n",
    "- `distinct()` ‚Üí requires shuffling duplicates across partitions.  \n",
    "- `join(df1, df2, \"col\")` ‚Üí rows with the same key must meet.  \n",
    "- `reduceByKey()` / `aggregateByKey()` (in RDDs).  \n",
    "\n",
    "üìä Problem (before shuffle):  \n",
    "![Wide Dependency Problem](./images/wide_dependecy_prob.png)\n",
    "\n",
    "üìä Solution (after shuffle + repartition):  \n",
    "![Wide Dependency Solution](./images/wide_denpendency_sol.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ‚ùì Your Doubt ‚Äî What if a Group is Too Big for One Partition?\n",
    "- Spark‚Äôs default shuffle partition size is **~128 MB** (not KB).  \n",
    "- After `groupBy()`, Spark ensures all rows of the same group key land in the **same partition**.  \n",
    "- But if one group is **larger than 128 MB**, what happens?\n",
    "\n",
    "### ‚úÖ The Answer:\n",
    "1. **Partition size is not a hard cap**  \n",
    "   - Spark can create partitions larger than 128 MB if needed.  \n",
    "   - The big group is stored in one partition, even if it exceeds the ‚Äúideal‚Äù size.\n",
    "\n",
    "2. **Aggregation correctness is preserved**  \n",
    "   - Example: `count()` runs within that partition only.  \n",
    "   - So logically, `count()` is still a **narrow dependency** (independent of other partitions).\n",
    "\n",
    "3. **Performance concerns (data skew)**  \n",
    "   - One executor may get a huge partition ‚Üí slower job.  \n",
    "   - Spark prevents crashes by spilling to disk if memory runs out.\n",
    "\n",
    "4. **Optimizations in practice**  \n",
    "   - Increase shuffle partitions (`spark.sql.shuffle.partitions`).  \n",
    "   - Apply *salting keys* to break very large groups into subgroups.  \n",
    "   - Use Spark‚Äôs **skew join optimization** (Spark 3+).\n",
    "\n",
    "üëâ So: correctness is always maintained, but performance may degrade if data skew is high.\n",
    "\n",
    "---\n",
    "\n",
    "# üìù Summary\n",
    "- **Narrow dependency**: map-style operations ‚Üí `select`, `filter`, `withColumn`, `map`, `count` (post-shuffle).  \n",
    "- **Wide dependency**: reduce-style operations ‚Üí `groupBy`, `orderBy`, `join`, `distinct`, `reduceByKey`.  \n",
    "- **Large group in wide dependency**: Spark allows big partitions ‚Üí results are correct, but skew can slow down jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011acd0",
   "metadata": {},
   "source": [
    "# Spark Execution Plan ‚Äî Jobs, Stages, and Tasks\n",
    "\n",
    "![Spark Execution Plan](./images/spark_execution_plan_.png)\n",
    "\n",
    "## Why an ‚Äúexecution plan‚Äù?\n",
    "Spark builds a **logical DAG** from your transformations (lazy).  \n",
    "When you run an **action** (e.g., `show()`, `count()`, `collect()`, `write`), Spark **materializes** that DAG into:\n",
    "1) **Jobs** ‚Üí split into  \n",
    "2) **Stages** (cut at shuffles) ‚Üí executed as many  \n",
    "3) **Tasks** (one per partition in that stage)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick definitions\n",
    "- **Job**: Triggered by **one action**. If your code has 3 actions, you‚Äôll see **3 jobs**.\n",
    "- **Stage**: A block of work that can run **without a shuffle** (only **narrow** deps). Stages are split at **wide** deps (e.g., `groupBy`, `join`, `orderBy`, `distinct`, `repartition`).\n",
    "- **Task**: A per-partition execution of a stage on an executor core.  \n",
    "  **#Tasks in a stage ‚âà #Partitions for that stage** (e.g., default `spark.sql.shuffle.partitions = 200`).\n",
    "\n",
    "---\n",
    "\n",
    "## From code ‚Üí plan (example)\n",
    "\n",
    "    def count_by_country(df):\n",
    "        return (\n",
    "            df.where(\"Age < 40\")                        # narrow\n",
    "              .select(\"Age\",\"Gender\",\"Country\",\"state\") # narrow\n",
    "              .groupBy(\"Country\")                       # wide ‚Üí shuffle\n",
    "              .count()                                  # aggregate after shuffle\n",
    "        )\n",
    "\n",
    "    survey_df   = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    partitioned = survey_df.repartition(2)              # wide (shuffle) when action triggers\n",
    "    result_df   = count_by_country(partitioned)\n",
    "\n",
    "    result = result_df.collect()                        # ACTION ‚Üí creates a Job\n",
    "\n",
    "**What Spark does:**\n",
    "- **Job A** (created by `.collect()`):\n",
    "  - **Stage 0 (map stage, narrow)**: scan + `where` + `select` on each input partition ‚Üí produces intermediate partitions.\n",
    "  - **Shuffle boundary** (because of `groupBy(\"Country\")`).\n",
    "  - **Stage 1 (reduce stage)**: shuffle-read all rows for each key; run aggregate (`count`) ‚Üí produces grouped result partitions.\n",
    "  - **Collect** pulls final partitions to driver (no extra shuffle).\n",
    "\n",
    "If you later do `result_df.write.parquet(...)`, that is **another action ‚Üí another Job**, with its own stages.\n",
    "\n",
    "---\n",
    "\n",
    "## Narrow vs Wide in the plan\n",
    "- **Narrow ops** (stay within a stage; no shuffle):  \n",
    "  `select`, `filter/where`, `withColumn`, map-style ops, and post-shuffle per-partition `count`.\n",
    "- **Wide ops** (cause shuffle ‚Üí new stage):  \n",
    "  `groupBy`/aggregations, `join`, `orderBy/sort`, `distinct`, `repartition`, `reduceByKey`/`aggregateByKey` (RDD).\n",
    "\n",
    "---\n",
    "\n",
    "## How tasks run\n",
    "- Each stage spawns **one task per partition**.\n",
    "- Tasks run in parallel across **executor cores**.  \n",
    "  Example: 5 executors √ó 5 cores = **25 concurrent tasks** max.  \n",
    "- If a stage has 100 partitions, Spark runs them in **waves** (25 at a time until 100 complete).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89ba49",
   "metadata": {},
   "source": [
    "# How This Spark Code Executes \n",
    "\n",
    "![Execution Plan (Jobs)](./images/ex_plan.png)\n",
    "\n",
    "## 1) Why are Jobs created during `read`?\n",
    "- `spark.read.csv(...)` is usually **lazy**, but certain options force Spark to **touch data**:\n",
    "  - `inferSchema=true` ‚Üí Spark must scan sample rows to determine column types.\n",
    "- This triggers internal work that shows up as **Jobs** in the UI:\n",
    "  - **Job 0**: file listing / metadata scan\n",
    "  - **Job 1**: schema inference scan\n",
    "> You didn‚Äôt call an action yet, but these options require data inspection, so Spark runs small internal jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Why `repartition(2)` appears first in the collect-job\n",
    "- `repartition(2)` is a **wide transformation** (requires **shuffle**) that sets the target partition count to **2**.\n",
    "- It‚Äôs **lazy** when written, but at the next action Spark inserts a **stage boundary** here:\n",
    "  - Stage for **repartition ‚Üí shuffle write** so later stages run with 2 partitions.\n",
    "- Reasons to do this in demos:\n",
    "  - Make parallelism explicit/controlled (2 tasks instead of many).\n",
    "  - Keep the UI simple to read.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Narrow ops before `groupBy`\n",
    "- `filter(\"Age < 40\")` and `select(\"Age\",\"Gender\",\"Country\",\"state\")` are **narrow** transformations.\n",
    "- Narrow ops run **within the same stage** (no shuffle).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) How `groupBy().count()` is planned\n",
    "- `groupBy(\"Country\")` is **wide**: all rows of the same key must meet ‚Üí **shuffle** ‚Üí **new stage**.\n",
    "- `.count()` **after `groupBy` is a transformation** (aggregation), **not an action**.\n",
    "- So aggregation happens **inside the post-shuffle stage**, and only the final **`collect()`** is the action.\n",
    "\n",
    "---\n",
    "\n",
    "![Stage Breakdown](./images/stage.png)\n",
    "\n",
    "### Putting it together (the collect job)\n",
    "1. **Stage 2 ‚Äî Repartition**\n",
    "   - Do `repartition(2)` ‚Üí **shuffle write to Exchange**.\n",
    "2. **Stage 3 ‚Äî Narrow ops + group keying**\n",
    "   - **Read Exchange** ‚Üí `where` ‚Üí `select` ‚Üí start `groupBy` ‚Üí **shuffle write**.\n",
    "3. **Stage 4 ‚Äî Aggregation + Collect**\n",
    "   - **Read Exchange** ‚Üí per-key `count()` (aggregation) ‚Üí driver **`collect()`**.\n",
    "\n",
    "**Tasks per stage:** ‚âà **# partitions** for that stage (here 1 in Stage 2‚Äôs input, then 2 after repartition, so Stage 3 and 4 show **2 tasks** in parallel).\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR\n",
    "- Jobs during `read` appear because `inferSchema` forces scans (internal actions).\n",
    "- `repartition(2)` is a shuffle point; Spark inserts it as the **first stage** of the collect job to set parallelism.\n",
    "- `groupBy` causes another shuffle (new stage); `count()` here is an **aggregation transformation**, not an action.\n",
    "- The **action** is `collect()` ‚Üí it triggers the entire **Job** made of those **Stages**, each executed as many **Tasks** as partitions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
