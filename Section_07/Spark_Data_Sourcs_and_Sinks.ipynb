{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd8dccf",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    " - [Data Lake, Data Sources & Data Sinks](#data-lake-data-sources--data-sinks)\n",
    " - [DataframeReader-API](#spark-dataframereader-api--quick-setup-with-visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211af2b",
   "metadata": {},
   "source": [
    "# Data Lake, Data Sources & Data Sinks\n",
    "![data_lake](./images/data_lake.png)\n",
    "## ðŸ”¹ What is a Data Lake?\n",
    "A **Data Lake** is a central storage system that can hold **structured, semi-structured, and unstructured data** in its **raw format**.  \n",
    "- Usually built on **distributed storage** like HDFS, Amazon S3, Azure Data Lake Storage, or Google Cloud Storage.  \n",
    "- Designed for **scalability, cost-efficiency, and flexibility**.  \n",
    "- Uses **schema-on-read** (apply schema only when processing).  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why Databases & Warehouses are NOT Data Lakes\n",
    "- **Databases** (e.g., MySQL, Oracle, PostgreSQL):  \n",
    "  - Optimized for **transactions (OLTP)**.  \n",
    "  - Store only **structured data**.  \n",
    "  - Capacity limited, not meant for petabytes of logs/images.  \n",
    "\n",
    "- **Data Warehouses** (e.g., Redshift, Snowflake, BigQuery):  \n",
    "  - Optimized for **analytics (OLAP)**.  \n",
    "  - Store **clean, structured, aggregated data**.  \n",
    "  - Require schema **before loading (schema-on-write)**.  \n",
    "  - Expensive for raw/unstructured data storage.  \n",
    "\n",
    "ðŸ“Œ Hence, they are not considered Data Lakes. They are **external systems** that may send data to or receive data from the lake.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Data Sources vs Data Sinks\n",
    "- **Data Source:** Where the data comes from before Spark processes it.  \n",
    "- **Data Sink:** Where the data goes after Spark processes it.  \n",
    "\n",
    "ðŸ‘‰ **Rule of Thumb:**  \n",
    "- **Reading â†’ Source**  \n",
    "- **Writing â†’ Sink**  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Internal vs External Storage (from a Data Lake POV)\n",
    "- **Internal Storage:**  \n",
    "  - Data already inside the Data Lake.  \n",
    "  - Examples: HDFS, S3, ADLS, GCS, Spark SQL tables, Delta Lake tables.  \n",
    "  - File formats: CSV, JSON, Parquet, Avro, ORC.  \n",
    "\n",
    "- **External Storage:**  \n",
    "  - Data that exists **outside the Data Lake**.  \n",
    "  - Examples:  \n",
    "    - **Databases:** MySQL, Oracle, SQL Server (via JDBC).  \n",
    "    - **NoSQL:** MongoDB, Cassandra.  \n",
    "    - **Cloud Warehouses:** Redshift, Snowflake, BigQuery.  \n",
    "    - **Streams:** Kafka.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "- **Data Lake** = Distributed storage system for raw + processed data (internal).  \n",
    "- **Databases/Warehouses** = Specialized external systems for transactions/analytics.  \n",
    "- **Internal Storage** = Files and cloud-based storage inside the Data Lake.  \n",
    "- **External Storage** = Databases, warehouses, and streaming systems outside the Data Lake.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac57",
   "metadata": {},
   "source": [
    "# Spark DataFrameReader API â€” Quick Setup (with Visual)\n",
    "\n",
    "![Spark Data Source API](./images/dataframe_api.png)\n",
    "\n",
    "## âœ… Standard Pattern\n",
    "Use the **same, consistent structure** for every source (CSV/JSON/Parquet/JDBC, etc.):\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"<source>\") \\        # \"csv\" | \"json\" | \"parquet\" | \"jdbc\" | ...\n",
    "    .option(\"<key>\", \"<value>\") \\# source-specific options\n",
    "    .schema(mySchema) \\          # optional (you can infer for many formats)\n",
    "    .load(\"<path-or-uri>\")       # files dir, single file, or JDBC parameters\n",
    "```\n",
    "\n",
    "### Key Parts\n",
    "- **`.format()`** â†’ declares the data source/connector.  \n",
    "- **`.option(k, v)`** â†’ controls how to read (header, delimiter, multiline, etc.).  \n",
    "- **`.schema()`** â†’ explicit schema (faster, safer than inference).  \n",
    "- **`.load()`** â†’ reads and returns a **DataFrame**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§° Read Modes for Semi-Structured Files (CSV/JSON/XML)\n",
    "Set with `.option(\"mode\", \"...\")`:\n",
    "- **`PERMISSIVE`** *(default)* â†’ keeps bad rows; puts the raw text in `_corrupt_record`.  \n",
    "- **`DROPMALFORMED`** â†’ drops malformed rows.  \n",
    "- **`FAILFAST`** â†’ aborts on first malformed row.\n",
    "\n",
    "> Tip: Pair with `.option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")` if you need a custom column name.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Minimal, Practical Examples\n",
    "\n",
    "### 1) CSV\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "csv_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(mySchema) \\\n",
    "    .load(\"/data/mycsvfiles/\")    # folder or file path\n",
    "```\n",
    "\n",
    "### 2) JSON\n",
    "```python\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .load(\"/data/events/*.json\")\n",
    "```\n",
    "\n",
    "### 3) Parquet (schema embedded; fastest, columnar)\n",
    "```python\n",
    "pq_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"/data/clean/parquet/\")\n",
    "```\n",
    "\n",
    "### 4) JDBC (example)\n",
    "```python\n",
    "jdbc_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host:5432/db\") \\\n",
    "    .option(\"dbtable\", \"public.orders\") \\\n",
    "    .option(\"user\", \"<USER>\") \\\n",
    "    .option(\"password\", \"<PASS>\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ž Checklist (Production-friendly)\n",
    "- Prefer **standard API** (`format â†’ option â†’ schema â†’ load`) over shortcuts (`.csv()`, `.json()`).\n",
    "- **Define schemas** for CSV/JSON to avoid inference surprises and speed up reads.\n",
    "- Set an explicit **`mode`** for resilient ingestion (PERMISSIVE for exploration, FAILFAST for strict jobs).\n",
    "- Keep paths/environment **config-driven** (donâ€™t hardcode).\n",
    "- Validate data with small samples before full runs (`df.limit(10).show(truncate=False)`).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
