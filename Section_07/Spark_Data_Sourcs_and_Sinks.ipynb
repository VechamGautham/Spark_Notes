{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd8dccf",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    " - [Data Lake, Data Sources & Data Sinks](#data-lake-data-sources--data-sinks)\n",
    " - [DataframeReader-API](#spark-dataframereader-api--quick-setup-with-visual)\n",
    " - [Spark DataframeSchema](#spark-dataframe-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211af2b",
   "metadata": {},
   "source": [
    "# Data Lake, Data Sources & Data Sinks\n",
    "![data_lake](./images/data_lake.png)\n",
    "## üîπ What is a Data Lake?\n",
    "A **Data Lake** is a central storage system that can hold **structured, semi-structured, and unstructured data** in its **raw format**.  \n",
    "- Usually built on **distributed storage** like HDFS, Amazon S3, Azure Data Lake Storage, or Google Cloud Storage.  \n",
    "- Designed for **scalability, cost-efficiency, and flexibility**.  \n",
    "- Uses **schema-on-read** (apply schema only when processing).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Databases & Warehouses are NOT Data Lakes\n",
    "- **Databases** (e.g., MySQL, Oracle, PostgreSQL):  \n",
    "  - Optimized for **transactions (OLTP)**.  \n",
    "  - Store only **structured data**.  \n",
    "  - Capacity limited, not meant for petabytes of logs/images.  \n",
    "\n",
    "- **Data Warehouses** (e.g., Redshift, Snowflake, BigQuery):  \n",
    "  - Optimized for **analytics (OLAP)**.  \n",
    "  - Store **clean, structured, aggregated data**.  \n",
    "  - Require schema **before loading (schema-on-write)**.  \n",
    "  - Expensive for raw/unstructured data storage.  \n",
    "\n",
    "üìå Hence, they are not considered Data Lakes. They are **external systems** that may send data to or receive data from the lake.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Data Sources vs Data Sinks\n",
    "- **Data Source:** Where the data comes from before Spark processes it.  \n",
    "- **Data Sink:** Where the data goes after Spark processes it.  \n",
    "\n",
    "üëâ **Rule of Thumb:**  \n",
    "- **Reading ‚Üí Source**  \n",
    "- **Writing ‚Üí Sink**  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Internal vs External Storage (from a Data Lake POV)\n",
    "- **Internal Storage:**  \n",
    "  - Data already inside the Data Lake.  \n",
    "  - Examples: HDFS, S3, ADLS, GCS, Spark SQL tables, Delta Lake tables.  \n",
    "  - File formats: CSV, JSON, Parquet, Avro, ORC.  \n",
    "\n",
    "- **External Storage:**  \n",
    "  - Data that exists **outside the Data Lake**.  \n",
    "  - Examples:  \n",
    "    - **Databases:** MySQL, Oracle, SQL Server (via JDBC).  \n",
    "    - **NoSQL:** MongoDB, Cassandra.  \n",
    "    - **Cloud Warehouses:** Redshift, Snowflake, BigQuery.  \n",
    "    - **Streams:** Kafka.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- **Data Lake** = Distributed storage system for raw + processed data (internal).  \n",
    "- **Databases/Warehouses** = Specialized external systems for transactions/analytics.  \n",
    "- **Internal Storage** = Files and cloud-based storage inside the Data Lake.  \n",
    "- **External Storage** = Databases, warehouses, and streaming systems outside the Data Lake.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac57",
   "metadata": {},
   "source": [
    "# Spark DataFrameReader API ‚Äî Quick Setup (with Visual)\n",
    "\n",
    "![Spark Data Source API](./images/dataframe_api.png)\n",
    "\n",
    "## ‚úÖ Standard Pattern\n",
    "Use the **same, consistent structure** for every source (CSV/JSON/Parquet/JDBC, etc.):\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"<source>\") \\        # \"csv\" | \"json\" | \"parquet\" | \"jdbc\" | ...\n",
    "    .option(\"<key>\", \"<value>\") \\# source-specific options\n",
    "    .schema(mySchema) \\          # optional (you can infer for many formats)\n",
    "    .load(\"<path-or-uri>\")       # files dir, single file, or JDBC parameters\n",
    "```\n",
    "\n",
    "### Key Parts\n",
    "- **`.format()`** ‚Üí declares the data source/connector.  \n",
    "- **`.option(k, v)`** ‚Üí controls how to read (header, delimiter, multiline, etc.).  \n",
    "- **`.schema()`** ‚Üí explicit schema (faster, safer than inference).  \n",
    "- **`.load()`** ‚Üí reads and returns a **DataFrame**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Read Modes for Semi-Structured Files (CSV/JSON/XML)\n",
    "Set with `.option(\"mode\", \"...\")`:\n",
    "- **`PERMISSIVE`** *(default)* ‚Üí keeps bad rows; puts the raw text in `_corrupt_record`.  \n",
    "- **`DROPMALFORMED`** ‚Üí drops malformed rows.  \n",
    "- **`FAILFAST`** ‚Üí aborts on first malformed row.\n",
    "\n",
    "> Tip: Pair with `.option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")` if you need a custom column name.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Minimal, Practical Examples\n",
    "\n",
    "### 1) CSV\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "csv_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(mySchema) \\\n",
    "    .load(\"/data/mycsvfiles/\")    # folder or file path\n",
    "```\n",
    "\n",
    "### 2) JSON\n",
    "```python\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .load(\"/data/events/*.json\")\n",
    "```\n",
    "\n",
    "### 3) Parquet (schema embedded; fastest, columnar)\n",
    "```python\n",
    "pq_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"/data/clean/parquet/\")\n",
    "```\n",
    "\n",
    "### 4) JDBC (example)\n",
    "```python\n",
    "jdbc_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host:5432/db\") \\\n",
    "    .option(\"dbtable\", \"public.orders\") \\\n",
    "    .option(\"user\", \"<USER>\") \\\n",
    "    .option(\"password\", \"<PASS>\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Checklist (Production-friendly)\n",
    "- Prefer **standard API** (`format ‚Üí option ‚Üí schema ‚Üí load`) over shortcuts (`.csv()`, `.json()`).\n",
    "- **Define schemas** for CSV/JSON to avoid inference surprises and speed up reads.\n",
    "- Set an explicit **`mode`** for resilient ingestion (PERMISSIVE for exploration, FAILFAST for strict jobs).\n",
    "- Keep paths/environment **config-driven** (don‚Äôt hardcode).\n",
    "- Validate data with small samples before full runs (`df.limit(10).show(truncate=False)`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdcd36",
   "metadata": {},
   "source": [
    "# Spark DataFrame Schema\n",
    "\n",
    "![Spark Data Types](./images/spark_data_types.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Define Schema?\n",
    "- Schema inference (like in CSV/JSON) is **not always reliable**.  \n",
    "- Defining schema explicitly ensures **correct column names** and **data types**.  \n",
    "- Spark provides two main ways:\n",
    "  1. **Programmatically using StructType & StructField**  \n",
    "  2. **Using a DDL (Data Definition Language) String**  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Programmatic Schema (StructType)\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", DateType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"FL_NUM\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with explicit schema\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\  # specify date parsing format\n",
    "    .schema(schema) \\\n",
    "    .load(\"/data/flights.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Schema with DDL String\n",
    "\n",
    "```python\n",
    "# Define schema using DDL string\n",
    "ddl_schema = \"FL_DATE DATE, AIRLINE STRING, FL_NUM INT\"\n",
    "\n",
    "# Read JSON with explicit DDL schema\n",
    "df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .schema(ddl_schema) \\\n",
    "    .load(\"/data/flights.json\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- **StructType/StructField** ‚Üí More explicit, flexible, IDE-friendly.  \n",
    "- **DDL String** ‚Üí Short and simple for quick schema definitions.  \n",
    "- Always set **`dateFormat`** (or timestamp format) when working with date/time fields in CSV/JSON.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018440bd",
   "metadata": {},
   "source": [
    "# üìò Spark Data Sink API ‚Äî Writing DataFrames\n",
    "\n",
    "![Spark Data Sink API](./images/dataframewriter.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Introduction\n",
    "In Spark, you write data using the **DataFrameWriter API**.  \n",
    "You access it with the `.write` method on a DataFrame.  \n",
    "This API is standardized and works with both **internal formats** (Parquet, JSON, CSV, ORC, Avro) and **external sinks** (JDBC, Cassandra, MongoDB, Kafka, Delta Lake, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ General Structure\n",
    "```python\n",
    "df.write \\\n",
    "  .format(\"parquet\") \\         # Output format (default = Parquet)\n",
    "  .mode(\"overwrite\") \\         # Save mode: append/overwrite/errorIfExists/ignore\n",
    "  .option(\"path\", \"/data/out\") # Target location or config\n",
    "  .save()\n",
    "```\n",
    "\n",
    "### Write Modes\n",
    "- **append** ‚Üí Add new data alongside existing files.  \n",
    "- **overwrite** ‚Üí Delete old data and write fresh output.  \n",
    "- **errorIfExists** ‚Üí Fail if data already exists.  \n",
    "- **ignore** ‚Üí Do nothing if data already exists.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Layout Controls\n",
    "Beyond format and mode, Spark gives you tools to control **how data is organized and stored**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) **Repartition**\n",
    "- **What it does:** Changes the number of partitions ‚Üí controls number of output files.  \n",
    "- **Why use:** Balance file counts, manage parallelism.  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  df.repartition(5).write.parquet(\"/data/output/repartitioned/\")\n",
    "  ```\n",
    "- **Advantage:** Prevents too many small files or one giant file.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) **partitionBy**\n",
    "- **What it does:** Creates **folders by column values**.  \n",
    "- **Why use:** Logical data organization + faster queries (partition pruning).  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  df.write \\\n",
    "    .partitionBy(\"country\", \"state\") \\\n",
    "    .parquet(\"/data/output/partitioned/\")\n",
    "  ```\n",
    "- **Advantage:** Queries can scan only needed partitions (`WHERE country='US'` ‚Üí reads only US folder).  \n",
    "- **Caution:** Too many unique values = too many small folders/files.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) **bucketBy** (for Spark-managed tables)\n",
    "- **What it does:** Hashes rows into a fixed number of buckets (files).  \n",
    "- **Why use:** Optimize **joins** and **groupBy** on the bucketed column.  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  df.write \\\n",
    "    .bucketBy(8, \"user_id\") \\\n",
    "    .sortBy(\"user_id\") \\\n",
    "    .saveAsTable(\"bucketed_users\")\n",
    "  ```\n",
    "- **Advantage:** Spark can skip shuffle during joins when both tables share same bucket column + bucket count.  \n",
    "- **Caution:** Only works for managed tables, not plain files.\n",
    "\n",
    "- üîπ Bucket Column\n",
    "-\t‚Ä¢\tThe bucket column is the column you tell Spark to use when distributing rows into buckets.\n",
    "- \t‚Ä¢\tSpark takes the value of that column, applies a hash function, and then assigns the row to a bucket.\n",
    "\n",
    "- üîπ Bucket Count\n",
    "- \t‚Ä¢\tThe bucket count is simply the number of buckets (output files) you want Spark to create.\n",
    "- \t‚Ä¢\tIt‚Äôs the 8 in the above example.\n",
    "- \t‚Ä¢\tSpark ensures that all rows are spread across these 8 buckets based on the hash of user_id.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4) **sortBy**\n",
    "- **What it does:** Sorts rows inside each bucket.  \n",
    "- **Why use:** Improves range queries, window functions, and some joins.  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  df.write \\\n",
    "    .bucketBy(8, \"user_id\") \\\n",
    "    .sortBy(\"timestamp\") \\\n",
    "    .saveAsTable(\"bucketed_sorted_users\")\n",
    "  ```\n",
    "- **Advantage:** Data is ordered within buckets ‚Üí faster queries on ranges.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) **maxRecordsPerFile**\n",
    "- **What it does:** Limits number of rows per file.  \n",
    "- **Why use:** Prevent very large files, balance parallelism.  \n",
    "- **Example:**\n",
    "  ```python\n",
    "  df.write \\\n",
    "    .option(\"maxRecordsPerFile\", 1000000) \\\n",
    "    .parquet(\"/data/output/maxrecords/\")\n",
    "  ```\n",
    "- **Advantage:** Avoids giant files (hard to read) and ensures manageable chunk sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- Use **write modes** to control overwrite/append behavior.  \n",
    "- Use **repartition** to manage file counts.  \n",
    "- Use **partitionBy** for logical folder-based organization.  \n",
    "- Use **bucketBy + sortBy** for optimized joins and range queries.  \n",
    "- Use **maxRecordsPerFile** to avoid massive files and keep outputs balanced.  \n",
    "\n",
    "Together, these make your Spark writes more **efficient, organized, and query-friendly**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01462f6",
   "metadata": {},
   "source": [
    "# ü™£ Bucketing with Two Tables in Spark\n",
    "\n",
    "## üîπ Scenario\n",
    "You have **two tables** (`Table A` and `Table B`) and you want to join them on `user_id`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Applying Bucketing\n",
    "- You bucket **Table A** with:\n",
    "  ```python\n",
    "  dfA.write.bucketBy(4, \"user_id\").saveAsTable(\"tableA\")\n",
    "  ```\n",
    "  ‚Üí Each row in Table A goes to one of **4 buckets** based on:\n",
    "  ```\n",
    "  bucket_number = hash(user_id) % 4\n",
    "  ```\n",
    "\n",
    "- You bucket **Table B** with:\n",
    "  ```python\n",
    "  dfB.write.bucketBy(4, \"user_id\").saveAsTable(\"tableB\")\n",
    "  ```\n",
    "  ‚Üí Same rule is applied for Table B.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What Happens?\n",
    "- **Each table has its own set of bucket files** (they don‚Äôt share physical files).  \n",
    "- But since both use the **same column (`user_id`)** and the **same bucket count (4)**,  \n",
    "  the same `user_id` values end up in the **same bucket number** in both tables.  \n",
    "\n",
    "üëâ Example with 4 buckets (simplified hash = user_id):\n",
    "- `user_id = 1 ‚Üí 1 % 4 = 1 ‚Üí Bucket 1 in both Table A and B`  \n",
    "- `user_id = 2 ‚Üí 2 % 4 = 2 ‚Üí Bucket 2 in both Table A and B`  \n",
    "- `user_id = 3 ‚Üí 3 % 4 = 3 ‚Üí Bucket 3 in both Table A and B`  \n",
    "- `user_id = 4 ‚Üí 4 % 4 = 0 ‚Üí Bucket 0 in both Table A and B`\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why is This Useful?\n",
    "When Spark performs a join:\n",
    "- It knows **Bucket 0 of Table A only needs to join with Bucket 0 of Table B**.  \n",
    "- Same for Bucket 1 with Bucket 1, and so on.  \n",
    "\n",
    "‚ö° This avoids a **full shuffle of all rows** (wide dependency).  \n",
    "Instead, the join happens **bucket-to-bucket** (narrower dependency).  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Point\n",
    "- **Table B does not append to Table A‚Äôs buckets**.  \n",
    "- Each table keeps its own files, but **follows the same bucketing logic**.  \n",
    "- This alignment is enough for Spark to optimize joins and reduce shuffle dramatically."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
