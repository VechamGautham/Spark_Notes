{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd8dccf",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    " - [Data Lake, Data Sources & Data Sinks](#data-lake-data-sources--data-sinks)\n",
    " - [DataframeReader-API](#spark-dataframereader-api--quick-setup-with-visual)\n",
    " - [Spark DataframeSchema](#spark-dataframe-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211af2b",
   "metadata": {},
   "source": [
    "# Data Lake, Data Sources & Data Sinks\n",
    "![data_lake](./images/data_lake.png)\n",
    "## üîπ What is a Data Lake?\n",
    "A **Data Lake** is a central storage system that can hold **structured, semi-structured, and unstructured data** in its **raw format**.  \n",
    "- Usually built on **distributed storage** like HDFS, Amazon S3, Azure Data Lake Storage, or Google Cloud Storage.  \n",
    "- Designed for **scalability, cost-efficiency, and flexibility**.  \n",
    "- Uses **schema-on-read** (apply schema only when processing).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Databases & Warehouses are NOT Data Lakes\n",
    "- **Databases** (e.g., MySQL, Oracle, PostgreSQL):  \n",
    "  - Optimized for **transactions (OLTP)**.  \n",
    "  - Store only **structured data**.  \n",
    "  - Capacity limited, not meant for petabytes of logs/images.  \n",
    "\n",
    "- **Data Warehouses** (e.g., Redshift, Snowflake, BigQuery):  \n",
    "  - Optimized for **analytics (OLAP)**.  \n",
    "  - Store **clean, structured, aggregated data**.  \n",
    "  - Require schema **before loading (schema-on-write)**.  \n",
    "  - Expensive for raw/unstructured data storage.  \n",
    "\n",
    "üìå Hence, they are not considered Data Lakes. They are **external systems** that may send data to or receive data from the lake.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Data Sources vs Data Sinks\n",
    "- **Data Source:** Where the data comes from before Spark processes it.  \n",
    "- **Data Sink:** Where the data goes after Spark processes it.  \n",
    "\n",
    "üëâ **Rule of Thumb:**  \n",
    "- **Reading ‚Üí Source**  \n",
    "- **Writing ‚Üí Sink**  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Internal vs External Storage (from a Data Lake POV)\n",
    "- **Internal Storage:**  \n",
    "  - Data already inside the Data Lake.  \n",
    "  - Examples: HDFS, S3, ADLS, GCS, Spark SQL tables, Delta Lake tables.  \n",
    "  - File formats: CSV, JSON, Parquet, Avro, ORC.  \n",
    "\n",
    "- **External Storage:**  \n",
    "  - Data that exists **outside the Data Lake**.  \n",
    "  - Examples:  \n",
    "    - **Databases:** MySQL, Oracle, SQL Server (via JDBC).  \n",
    "    - **NoSQL:** MongoDB, Cassandra.  \n",
    "    - **Cloud Warehouses:** Redshift, Snowflake, BigQuery.  \n",
    "    - **Streams:** Kafka.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- **Data Lake** = Distributed storage system for raw + processed data (internal).  \n",
    "- **Databases/Warehouses** = Specialized external systems for transactions/analytics.  \n",
    "- **Internal Storage** = Files and cloud-based storage inside the Data Lake.  \n",
    "- **External Storage** = Databases, warehouses, and streaming systems outside the Data Lake.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bac57",
   "metadata": {},
   "source": [
    "# Spark DataFrameReader API ‚Äî Quick Setup (with Visual)\n",
    "\n",
    "![Spark Data Source API](./images/dataframe_api.png)\n",
    "\n",
    "## ‚úÖ Standard Pattern\n",
    "Use the **same, consistent structure** for every source (CSV/JSON/Parquet/JDBC, etc.):\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"<source>\") \\        # \"csv\" | \"json\" | \"parquet\" | \"jdbc\" | ...\n",
    "    .option(\"<key>\", \"<value>\") \\# source-specific options\n",
    "    .schema(mySchema) \\          # optional (you can infer for many formats)\n",
    "    .load(\"<path-or-uri>\")       # files dir, single file, or JDBC parameters\n",
    "```\n",
    "\n",
    "### Key Parts\n",
    "- **`.format()`** ‚Üí declares the data source/connector.  \n",
    "- **`.option(k, v)`** ‚Üí controls how to read (header, delimiter, multiline, etc.).  \n",
    "- **`.schema()`** ‚Üí explicit schema (faster, safer than inference).  \n",
    "- **`.load()`** ‚Üí reads and returns a **DataFrame**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Read Modes for Semi-Structured Files (CSV/JSON/XML)\n",
    "Set with `.option(\"mode\", \"...\")`:\n",
    "- **`PERMISSIVE`** *(default)* ‚Üí keeps bad rows; puts the raw text in `_corrupt_record`.  \n",
    "- **`DROPMALFORMED`** ‚Üí drops malformed rows.  \n",
    "- **`FAILFAST`** ‚Üí aborts on first malformed row.\n",
    "\n",
    "> Tip: Pair with `.option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")` if you need a custom column name.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Minimal, Practical Examples\n",
    "\n",
    "### 1) CSV\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "csv_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(mySchema) \\\n",
    "    .load(\"/data/mycsvfiles/\")    # folder or file path\n",
    "```\n",
    "\n",
    "### 2) JSON\n",
    "```python\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .load(\"/data/events/*.json\")\n",
    "```\n",
    "\n",
    "### 3) Parquet (schema embedded; fastest, columnar)\n",
    "```python\n",
    "pq_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"/data/clean/parquet/\")\n",
    "```\n",
    "\n",
    "### 4) JDBC (example)\n",
    "```python\n",
    "jdbc_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host:5432/db\") \\\n",
    "    .option(\"dbtable\", \"public.orders\") \\\n",
    "    .option(\"user\", \"<USER>\") \\\n",
    "    .option(\"password\", \"<PASS>\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Checklist (Production-friendly)\n",
    "- Prefer **standard API** (`format ‚Üí option ‚Üí schema ‚Üí load`) over shortcuts (`.csv()`, `.json()`).\n",
    "- **Define schemas** for CSV/JSON to avoid inference surprises and speed up reads.\n",
    "- Set an explicit **`mode`** for resilient ingestion (PERMISSIVE for exploration, FAILFAST for strict jobs).\n",
    "- Keep paths/environment **config-driven** (don‚Äôt hardcode).\n",
    "- Validate data with small samples before full runs (`df.limit(10).show(truncate=False)`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdcd36",
   "metadata": {},
   "source": [
    "# Spark DataFrame Schema\n",
    "\n",
    "![Spark Data Types](./images/spark_data_types.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Define Schema?\n",
    "- Schema inference (like in CSV/JSON) is **not always reliable**.  \n",
    "- Defining schema explicitly ensures **correct column names** and **data types**.  \n",
    "- Spark provides two main ways:\n",
    "  1. **Programmatically using StructType & StructField**  \n",
    "  2. **Using a DDL (Data Definition Language) String**  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Programmatic Schema (StructType)\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", DateType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"FL_NUM\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with explicit schema\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\  # specify date parsing format\n",
    "    .schema(schema) \\\n",
    "    .load(\"/data/flights.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Schema with DDL String\n",
    "\n",
    "```python\n",
    "# Define schema using DDL string\n",
    "ddl_schema = \"FL_DATE DATE, AIRLINE STRING, FL_NUM INT\"\n",
    "\n",
    "# Read JSON with explicit DDL schema\n",
    "df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .schema(ddl_schema) \\\n",
    "    .load(\"/data/flights.json\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- **StructType/StructField** ‚Üí More explicit, flexible, IDE-friendly.  \n",
    "- **DDL String** ‚Üí Short and simple for quick schema definitions.  \n",
    "- Always set **`dateFormat`** (or timestamp format) when working with date/time fields in CSV/JSON.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
