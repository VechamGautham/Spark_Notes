{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7206ee7b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Big Data Problem & How it Was Solved](#big-data-problem--how-it-was-solved)\n",
    "- [Scripting Languages vs Programming Interfaces in Databases](#scripting-languages-vs-programming-interfaces-in-databases)\n",
    "- [Hadoop & its Core-Compoents](#hadoop--its-core-components)\n",
    "    - [YARN (Yet Another Resource Negotiator)](#yarn)\n",
    "    - [HDFS (Hadoop Distributed File System)](#hdfs-hadoop-distributed-file-system)\n",
    "    - [MapReduce](#mapreduce)\n",
    "- [History of Hadoop,Google Big Data Solutions & hive](#history-of-hadoop-googles-big-data-solutions--hive)\n",
    "- [From Hadoop & Hive -> Spark: Problems and Solutions](#from-hadoop--hive---spark-problems-and-solutions)\n",
    "- [Data Lakes](#data-lakes)\n",
    "- [Apache Spark Overview](#apache-spark-overview)\n",
    "    - [Why Apache Spark](#why-apache-spark)\n",
    "    - [Databricks](#databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc880e",
   "metadata": {},
   "source": [
    "# Big Data Problem & How It Was Solved  \n",
    "\n",
    "In the early days, data processing was handled by COBOL (1959) and later by RDBMS systems like Oracle, SQL Server, and MySQL.  \n",
    "These systems were designed for **structured data** stored in rows and columns. They provided SQL for querying, scripting languages (PL/SQL, T-SQL), and programming interfaces (JDBC/ODBC).  \n",
    "For decades, this worked perfectly.  \n",
    "\n",
    "But then data began to change.  \n",
    "We started seeing **semi-structured formats** like JSON and XML, and soon **unstructured data** such as PDFs, images, videos, and logs.  \n",
    "Businesses were now collecting **huge amounts of data (terabytes to petabytes)** at **very high speed** from internet, mobile apps, and social media.  \n",
    "\n",
    "This shift created the **Big Data problem**, defined by the 3Vs:  \n",
    "- **Volume** ‚Üí massive data sizes  \n",
    "- **Velocity** ‚Üí fast generation and the need for real-time/near real-time processing  \n",
    "- **Variety** ‚Üí structured, semi-structured, and unstructured formats  \n",
    "\n",
    "Traditional RDBMS was not built to handle these challenges.  \n",
    "\n",
    "---\n",
    "\n",
    "## Approaches to Big Data  \n",
    "Two main approaches were considered:  \n",
    "\n",
    "- **Monolithic systems** (e.g., Teradata, Exadata)  \n",
    "  These relied on one huge, powerful machine. They scaled vertically (adding more CPU, RAM, disk to the same box). While powerful, they were costly, hard to scale quickly, and not fault-tolerant‚Äîif hardware failed, the whole system went down.  \n",
    "\n",
    "- **Distributed systems**  \n",
    "  Instead of one large machine, this approach used a **cluster of many smaller machines** working together. They scaled horizontally (just add more machines), tolerated hardware failures (other machines kept running), and were far more economical.  \n",
    "\n",
    "---\n",
    "\n",
    "## Hadoop: The Solution üöÄ  \n",
    "The industry adopted the distributed approach, which led to the creation of **Hadoop**.  \n",
    "\n",
    "Hadoop acted like an **operating system for a cluster of computers**. It gave three key capabilities:  \n",
    "1. **Cluster Management** ‚Äì making many machines work as if they were one.  \n",
    "2. **Distributed Storage** ‚Äì storing structured, semi-structured, and unstructured data across the cluster.  \n",
    "3. **MapReduce Processing** ‚Äì running programs in parallel across machines for fast data processing.  \n",
    "\n",
    "Over time, an ecosystem grew around Hadoop:  \n",
    "- **Hive** allowed SQL-style queries.  \n",
    "- **Pig** gave a scripting language.  \n",
    "- **HBase** supported NoSQL storage.  \n",
    "- **Sqoop** handled data import/export.  \n",
    "- **Oozie** managed workflows.  \n",
    "\n",
    "---\n",
    "\n",
    "## RDBMS vs Hadoop  \n",
    "RDBMS remained strong for **small to medium structured data**, but Hadoop became the go-to platform for **large-scale, diverse, fast-moving data**.  \n",
    "It could scale to petabytes, handle all data types, and process information in parallel, making Big Data manageable.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Takeaway  \n",
    "The Big Data problem arose because of the **explosion of data in volume, velocity, and variety**.  \n",
    "RDBMS could not keep up, so the industry moved towards **distributed systems**.  \n",
    "Hadoop emerged as the practical solution‚Äîscalable, fault-tolerant, economical, and capable of handling modern data challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52262d0",
   "metadata": {},
   "source": [
    "# Scripting Languages vs Programming Interfaces in Databases\n",
    "\n",
    "## üîπ Scripting Languages (inside the DB engine)\n",
    "**What they are:** SQL extensions that add programming constructs so logic can run *inside* the database.\n",
    "**Common ones:** **PL/SQL** (Oracle), **T-SQL** (SQL Server)  \n",
    "**Why use them:** Encapsulate business rules near the data; support transactions, error handling, jobs, and stored procedures.\n",
    "\n",
    "**Features**\n",
    "- Variables, control flow (`IF`, loops)\n",
    "- Stored procedures, functions, triggers\n",
    "- Exceptions / error handling\n",
    "- Execute multiple SQL statements as one program\n",
    "\n",
    "**Mini-example (PL/SQL)**\n",
    "```sql\n",
    "DECLARE\n",
    "  total_sales NUMBER;\n",
    "BEGIN\n",
    "  SELECT SUM(amount) INTO total_sales FROM orders WHERE order_date = TRUNC(SYSDATE);\n",
    "  IF total_sales > 10000 THEN\n",
    "    DBMS_OUTPUT.PUT_LINE('High sales day');\n",
    "  ELSE\n",
    "    DBMS_OUTPUT.PUT_LINE('Normal');\n",
    "  END IF;\n",
    "END;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Programming Interfaces (from external applications)\n",
    "**What they are:** APIs/drivers that let applications (Java, Python, C#, etc.) connect to a DB, send SQL, and read results.\n",
    "**Common ones:** **ODBC** (language-agnostic standard), **JDBC** (Java-specific)  \n",
    "**Why use them:** Build apps/services that query/update the database, integrate with UIs, APIs, and batch jobs.\n",
    "\n",
    "**Typical flow**\n",
    "1. Open a connection (URL, user, password)\n",
    "2. Prepare/execute SQL\n",
    "3. Iterate result set\n",
    "4. Close resources\n",
    "\n",
    "**Mini-example (Java + JDBC)**\n",
    "```java\n",
    "import java.sql.*;\n",
    "class Demo {\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    try (Connection c = DriverManager.getConnection(\n",
    "           \"jdbc:mysql://localhost:3306/mydb\",\"user\",\"pass\");\n",
    "         PreparedStatement ps = c.prepareStatement(\"SELECT name FROM students\");\n",
    "         ResultSet rs = ps.executeQuery()) {\n",
    "      while (rs.next()) System.out.println(rs.getString(\"name\"));\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ One-line difference to remember\n",
    "- **Scripting languages** ‚Üí run **inside** the database (procedures/functions/triggers).\n",
    "- **Programming interfaces** ‚Üí connect **from outside** so your app can talk to the DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34663cce",
   "metadata": {},
   "source": [
    "# Hadoop & Its Core Components  \n",
    "\n",
    "Hadoop is a **distributed data processing platform**. It lets you use many computers (a cluster) as if they were **one single big computer**.  \n",
    "\n",
    "It has **three main functions** (layers):  \n",
    "\n",
    "![YARN](./images/yarn.png)\n",
    "\n",
    "1. **YARN (Yet Another Resource Negotiator)**  \n",
    "   - Acts like the **Operating System of the cluster**.  \n",
    "   - Manages resources (CPU, RAM) across all nodes.  \n",
    "   - Decides *which app runs where*.  \n",
    "\n",
    "2. **HDFS (Hadoop Distributed File System)**  \n",
    "   - Storage layer of Hadoop.  \n",
    "   - Splits files into blocks and spreads them across multiple machines.  \n",
    "   - Keeps copies (replicas) for fault tolerance.  \n",
    "\n",
    "3. **MapReduce**  \n",
    "   - Processing layer of Hadoop.  \n",
    "   - Breaks big jobs into smaller tasks and runs them in parallel across the cluster.  \n",
    "   - ‚ÄúMap‚Äù phase splits the work, ‚ÄúReduce‚Äù phase combines the results.  \n",
    "\n",
    "---\n",
    "\n",
    "# YARN   \n",
    "\n",
    "YARN = **Yet Another Resource Negotiator**  \n",
    "üëâ Think of it as the **traffic controller** of the Hadoop cluster.  \n",
    "\n",
    "## Why YARN?  \n",
    "- On a single computer, your **OS** decides how programs share CPU and RAM.  \n",
    "- In a cluster, you need the same thing ‚Äî a system to manage resources across **many computers**.  \n",
    "- That‚Äôs YARN‚Äôs job.  \n",
    "\n",
    "---\n",
    "\n",
    "## YARN Architecture  \n",
    "YARN has **3 main components**:  \n",
    "\n",
    "1. **Resource Manager (RM)**  \n",
    "   - Runs on the **master node**.  \n",
    "   - The global **boss** of the cluster.  \n",
    "   - Decides *which worker node will run an application*.  \n",
    "\n",
    "2. **Node Manager (NM)**  \n",
    "   - Runs on **every worker node**.  \n",
    "   - Local **site manager** for that machine.  \n",
    "   - Reports health and available CPU/RAM to RM.  \n",
    "   - Starts/stops application containers when told by RM.  \n",
    "\n",
    "3. **Application Master (AM)**  \n",
    "   - Created fresh for **each application** or **job**.  \n",
    "   - Lives inside a **container**.  \n",
    "   - Manages *just that one app*: requesting more containers, tracking tasks, reporting back.  \n",
    "\n",
    "---\n",
    "\n",
    "## Containers in YARN  \n",
    "- A **container = a mini-computer inside a worker node**.  \n",
    "- It bundles some CPU + RAM that only one app can use.  \n",
    "- Example: **2 CPU cores + 4 GB RAM** reserved for one container.  \n",
    "\n",
    "üëâ Multiple containers can run on the same worker node, as long as it has enough resources.  \n",
    "\n",
    "---\n",
    "\n",
    "## How an Application Runs on YARN (Step by Step)  \n",
    "\n",
    "1. **Submit Application**  \n",
    "   - You give your code (app) to the Resource Manager.  \n",
    "\n",
    "2. **Resource Manager Decides**  \n",
    "   - RM checks all Node Managers ‚Üí finds one with free CPU/RAM.  \n",
    "\n",
    "3. **Node Manager Starts a Container**  \n",
    "   - NM carves out a container (mini-computer) and launches an **Application Master** inside it.  \n",
    "\n",
    "4. **Application Master Runs App**  \n",
    "   - AM manages your app:  \n",
    "     - Requests more containers if needed.  \n",
    "     - Coordinates tasks across the cluster.  \n",
    "\n",
    "5. **Completion**  \n",
    "   - When tasks finish, AM informs RM.  \n",
    "   - Containers are released, resources become free.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Points to Remember  \n",
    "- **YARN = Cluster Resource Manager**.  \n",
    "- **3 components**:  \n",
    "  - RM = global boss  \n",
    "  - NM = site manager on each node  \n",
    "  - AM = per-app project manager  \n",
    "- **Container = mini-computer inside a node** (CPU + RAM slice).  \n",
    "- Each application always has at least one container (for its AM), and can request more for tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "#  HDFS (Hadoop Distributed File System)\n",
    "\n",
    "HDFS is Hadoop‚Äôs **storage system**.  \n",
    "It allows you to **save large files across a cluster of computers** and **read them back later**.  \n",
    "Just like your laptop has a file system (NTFS, ext4, etc.), a Hadoop cluster has its own ‚Üí **HDFS**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Components of HDFS\n",
    "![hdfs](./images/hdfs.png)\n",
    "\n",
    "1. **NameNode (Master)**\n",
    "   - Runs on the **master node**.\n",
    "   - Stores **metadata** (information *about* the files, not the data itself).\n",
    "   - Metadata includes:\n",
    "     - File name, directory\n",
    "     - File size\n",
    "     - Number of blocks\n",
    "     - Sequence/order of blocks\n",
    "     - Which DataNodes hold which blocks\n",
    "\n",
    "2. **DataNodes (Workers)**\n",
    "   - Run on **all worker nodes**.\n",
    "   - Store the **actual file data** as blocks.\n",
    "   - Periodically report back to the NameNode about their health and stored blocks.\n",
    "\n",
    "üëâ Analogy:  \n",
    "- **NameNode = Librarian** (knows where every page of a book is stored).  \n",
    "- **DataNodes = Shelves** (actually hold the book pages).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How Writing Works in HDFS\n",
    "\n",
    "1. User issues a file copy command (e.g., `hdfs dfs -put myfile.txt /data/`).\n",
    "2. Request goes to the **NameNode**.\n",
    "3. NameNode decides *which DataNodes* will store the file.\n",
    "4. File is **split into blocks** (default = 128 MB).\n",
    "5. Blocks are distributed across different DataNodes.\n",
    "\n",
    "**Example:**  \n",
    "A 512 MB file ‚Üí split into 4 blocks (128 MB each).  \n",
    "- Block 1 ‚Üí DataNode A  \n",
    "- Block 2 ‚Üí DataNode B  \n",
    "- Block 3 ‚Üí DataNode C  \n",
    "- Block 4 ‚Üí DataNode A (or another node)\n",
    "\n",
    "The **NameNode** keeps the ‚Äúmap‚Äù of where each block is stored.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How Reading Works in HDFS\n",
    "\n",
    "1. User issues a **read request**.\n",
    "2. The request goes to the **NameNode**.\n",
    "3. NameNode provides:\n",
    "   - Block IDs\n",
    "   - Order of blocks\n",
    "   - Which DataNodes hold them\n",
    "4. Client fetches blocks directly from DataNodes (in parallel).\n",
    "5. Client **reassembles blocks** into the original file.\n",
    "\n",
    "üëâ Flow:  \n",
    "**NameNode ‚Üí map of file**  \n",
    "**DataNodes ‚Üí actual file pieces**  \n",
    "**Client ‚Üí puts pieces back together**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Points to Remember\n",
    "- **HDFS stores files by breaking them into blocks** (default 128 MB).\n",
    "- **NameNode stores metadata** (file info and block locations).\n",
    "- **DataNodes store actual file blocks**.\n",
    "- **Write = file split ‚Üí blocks sent to DataNodes**.  \n",
    "- **Read = NameNode gives map ‚Üí DataNodes send blocks ‚Üí client rebuilds file**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd56961",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "MapReduce is both:  \n",
    "1. **A Programming Model** ‚Üí a way to break big problems into smaller steps (Map + Reduce).  \n",
    "2. **A Programming Framework** ‚Üí Hadoop‚Äôs system (YARN + HDFS + MapReduce engine) that runs your code across a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Programming Model (General Explanation)\n",
    "\n",
    "- **Map Stage**  \n",
    "  - Input data is split into blocks (128 MB by default).  \n",
    "  - Each block is processed independently by a `map()` function.  \n",
    "  - Mapper outputs intermediate results (e.g., partial counts).\n",
    "\n",
    "- **Reduce Stage**  \n",
    "  - Collects outputs from all mappers.  \n",
    "  - Aggregates them with a `reduce()` function.  \n",
    "  - Produces the final result.  \n",
    "\n",
    "üëâ **Map = parallel block-level work**  \n",
    "üëâ **Reduce = aggregation/consolidation**\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Programming Framework (General Explanation)\n",
    "\n",
    "- **Resource Manager (RM)**  \n",
    "  - Runs on the master node.  \n",
    "  - Global cluster boss.  \n",
    "  - Decides how many containers each job gets.  \n",
    "\n",
    "- **Application Master (AM)**  \n",
    "  - Runs once per job (inside a container on a worker node).  \n",
    "  - Job-level boss.  \n",
    "  - Requests containers from RM, launches tasks, monitors them.  \n",
    "\n",
    "- **Map Task**  \n",
    "  - One per block.  \n",
    "  - Runs the user‚Äôs `map()` function.  \n",
    "  - Processes actual data.  \n",
    "\n",
    "- **Reduce Task**  \n",
    "  - Collects mapper outputs.  \n",
    "  - Runs user‚Äôs `reduce()` function.  \n",
    "  - Aggregates results into final output.  \n",
    "\n",
    "üëâ **Flow**:  \n",
    "Client ‚Üí RM ‚Üí AM ‚Üí Map Tasks ‚Üí Reduce Task ‚Üí Final Output  \n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Example: Counting Lines in a 20 TB File\n",
    "\n",
    "### Step A ‚Äì Storage in HDFS\n",
    "- File size = **20 TB**  \n",
    "- Block size = **128 MB**  \n",
    "- Number of blocks = 20 TB √∑ 128 MB ‚âà **160,000 blocks**  \n",
    "- HDFS spreads blocks across 20 worker nodes.  \n",
    "\n",
    "### Step B ‚Äì MapReduce Execution\n",
    "![map_reduce](./images/map_reduce_ps.png)\n",
    "1. **Job submission**  \n",
    "   - Client submits code with `map()` and `reduce()`.  \n",
    "\n",
    "2. **RM allocates AM**  \n",
    "   - RM launches **1 Application Master** for this job.  \n",
    "\n",
    "3. **Map phase** \n",
    "![map](./images/map.png) \n",
    "   - AM requests ~160,000 containers (one per block).  \n",
    "   - Each container runs a Map task to count lines in its block.  \n",
    "   - Outputs partial counts:  \n",
    "     - Block 1 ‚Üí 1.2M lines  \n",
    "     - Block 2 ‚Üí 1.3M lines ‚Ä¶  \n",
    "\n",
    "4. **Reduce phase**  \n",
    "![reduce](./images/reduce.png)\n",
    "   - After all maps finish, AM requests a container for the Reduce task.  \n",
    "   - Reducer collects 160,000 partial counts.  \n",
    "   - Aggregates them ‚Üí final line count for the 20 TB file.  \n",
    "\n",
    "5. **Result**  \n",
    "   - Output is stored back in HDFS or returned to the client.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Roles Recap (General + Example)\n",
    "\n",
    "| Component | General Role | 20 TB Example |\n",
    "|-----------|--------------|----------------|\n",
    "| **RM** | Global cluster resource allocator | Gave containers for ~160,000 Map tasks + 1 Reduce task |\n",
    "| **AM** | Job coordinator | Managed this line-count job, monitored Map + Reduce tasks |\n",
    "| **Map Task** | Processes one block with `map()` | Each of 160,000 tasks counted lines in one block |\n",
    "| **Reduce Task** | Aggregates mapper outputs | Summed 160,000 partial counts into one total |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "- **General:** MapReduce splits work into `map()` (parallel per block) and `reduce()` (aggregation).  \n",
    "- **Framework:** RM (cluster boss), AM (job boss), Map tasks (block workers), Reduce tasks (final aggregator).  \n",
    "- **20 TB Example:** 160,000 Map tasks count lines block-by-block ‚Üí Reduce task adds them ‚Üí total line count produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f83f9",
   "metadata": {},
   "source": [
    "# History of Hadoop, Google‚Äôs Big Data Solutions & Hive\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Google‚Äôs Big Data Problem\n",
    "\n",
    "In the early 2000s, Google needed to build a **web search engine**.  \n",
    "They faced four massive challenges:\n",
    "\n",
    "1. **Data Collection (Web Crawling)**\n",
    "   - Problem: Needed to fetch billions of web pages from the internet.  \n",
    "   - Solution: Built large-scale **web crawlers** to download HTML + metadata.\n",
    "\n",
    "2. **Data Storage (Petabyte Scale)**\n",
    "   - Problem: Too much data for a single machine ‚Üí traditional databases failed.  \n",
    "   - Solution: Built the **Google File System (GFS, 2003)**.  \n",
    "     - Split files into chunks (64 MB).  \n",
    "     - Distributed across many cheap machines.  \n",
    "     - Replicated chunks for fault tolerance.\n",
    "\n",
    "3. **Data Processing (Computation Power)**\n",
    "   - Problem: Needed to run algorithms (like **PageRank**) on billions of pages + links.  \n",
    "   - Solution: Created the **MapReduce model (2004)**.  \n",
    "     - **Map** ‚Üí process small chunks of data in parallel.  \n",
    "     - **Reduce** ‚Üí aggregate results.  \n",
    "     - Allowed processing huge datasets in hours instead of months.\n",
    "\n",
    "4. **Data Serving (Querying Results)**\n",
    "   - Problem: Users needed instant answers to queries (‚Äúbest pizza in New York‚Äù).  \n",
    "   - Solution: Built **BigTable** (distributed, column-oriented DB).  \n",
    "     - Stored **inverted index**: words ‚Üí list of pages containing them.  \n",
    "     - Supported **random access lookups** across thousands of machines.  \n",
    "     - Combined with PageRank ‚Üí returned ranked results in milliseconds.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. From Google ‚Üí Hadoop\n",
    "\n",
    "- Google published **whitepapers**:\n",
    "  - **2003 ‚Üí GFS** (storage)\n",
    "  - **2004 ‚Üí MapReduce** (processing)\n",
    "\n",
    "- The **open-source community (Doug Cutting & Mike Cafarella)** created an open-source version:\n",
    "  - **HDFS (Hadoop Distributed File System)** ‚Üí based on GFS.  \n",
    "  - **Hadoop MapReduce Framework** ‚Üí based on Google‚Äôs MapReduce.\n",
    "\n",
    "üëâ Thus, **Hadoop was born**: an open-source big data platform inspired by Google‚Äôs systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Hive: Making Hadoop Easier\n",
    "\n",
    "- Problem: Writing **raw MapReduce jobs in Java** was **hard** and time-consuming.  \n",
    "- Solution: Facebook built **Hive** (later Apache Hive).  \n",
    "\n",
    "**Hive provided:**\n",
    "1. Ability to create **databases, tables, and views** on Hadoop data.  \n",
    "2. A **SQL-like language (HiveQL)** to query big datasets.  \n",
    "3. An engine that **translated SQL ‚Üí MapReduce jobs** internally.  \n",
    "\n",
    "üëâ Developers familiar with SQL could now use Hadoop **without writing MapReduce code**.  \n",
    "üëâ Example:\n",
    "```sql\n",
    "SELECT COUNT(*) \n",
    "FROM logs \n",
    "WHERE status = 'ERROR';\n",
    "```\n",
    "Hive would turn this into a MapReduce workflow under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- **Google‚Äôs Innovations**\n",
    "  - **GFS ‚Üí storage system** for petabytes.  \n",
    "  - **MapReduce ‚Üí processing model** for parallel computation.  \n",
    "  - **BigTable ‚Üí fast query engine** for serving results.  \n",
    "\n",
    "- **Hadoop‚Äôs Implementations**\n",
    "  - **HDFS** (open-source GFS).  \n",
    "  - **Hadoop MapReduce Framework** (open-source MapReduce).  \n",
    "  - **HBase** (open-source version of BigTable).  \n",
    "\n",
    "- **Hive‚Äôs Role**\n",
    "  - Simplified Hadoop usage with SQL-like queries.  \n",
    "  - Made Hadoop accessible to the broader developer community.\n",
    "\n",
    "üëâ Hadoop = **Google‚Äôs ideas, made open-source**.  \n",
    "üëâ Hive = **SQL on Hadoop**, making big data processing much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0da2cb",
   "metadata": {},
   "source": [
    "# From Hadoop & Hive -> Spark: Problems and Solutions\n",
    "\n",
    "Hadoop (HDFS + MapReduce) and Hive were revolutionary,  \n",
    "but they faced critical limitations. Apache Spark was created to fix them.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Performance (Speed)\n",
    "\n",
    "**Problem with Hadoop/Hive**  \n",
    "- Hadoop MapReduce wrote **intermediate results to disk** after every stage.  \n",
    "- Heavy disk I/O slowed jobs down (hours for large datasets).  \n",
    "- Hive SQL queries were also slow because they ran on MapReduce engine.  \n",
    "\n",
    "**Spark‚Äôs Solution**  \n",
    "- Spark keeps intermediate data **in memory (RAM)** whenever possible.  \n",
    "- Only falls back to disk if needed.  \n",
    "- Result: **10x‚Äì100x faster** than MapReduce/Hive for most workloads.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Programming Complexity\n",
    "\n",
    "**Problem with Hadoop**  \n",
    "- Writing MapReduce programs in Java was complex and verbose.  \n",
    "- Even simple word count required dozens of lines of Java.  \n",
    "- Hive simplified queries with SQL, but still ran on slow MapReduce.  \n",
    "\n",
    "**Spark‚Äôs Solution**  \n",
    "- Spark introduced **high-level, composable APIs**:  \n",
    "  - `.map()`, `.filter()`, `.reduceByKey()`, `.join()`, `.groupBy()`  \n",
    "- Provided **Spark SQL** for SQL queries.  \n",
    "- Easier to write and much faster to run. \n",
    "- big thanks to in memory computing \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Language Support\n",
    "\n",
    "**Problem with Hadoop/Hive**  \n",
    "- Hadoop MapReduce was **Java-only**.  \n",
    "- Hive SQL was easier, but lacked flexibility for advanced tasks (ML, streaming).  \n",
    "\n",
    "**Spark‚Äôs Solution**  \n",
    "- Multi-language support: **Scala, Java, Python (PySpark), R (SparkR)**.  \n",
    "- Opened big data processing to data scientists and analysts, not just Java engineers.  \n",
    "\n",
    "\n",
    "\n",
    "Hadoop MapReduce was **Java-only** and rigid.  \n",
    "- Writing iterative algorithms for **machine learning** was painful: each iteration became a new MapReduce job, with results written to **disk** every time ‚Üí extremely slow.  \n",
    "- Hadoop also couldn‚Äôt handle **streaming data** (only batch processing).  \n",
    "\n",
    "Spark solved this with **multi-language support** (Scala, Java, Python, R) and an **in-memory engine**.  \n",
    "- ML algorithms reuse data in **RAM** across iterations ‚Üí 10‚Äì100x faster than Hadoop.  \n",
    "- Spark introduced **Structured Streaming**, enabling near real-time analytics, impossible in Hadoop‚Äôs disk-based batch model.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "| Feature              | Hadoop/Hive                         | Spark                                      |\n",
    "|----------------------|-------------------------------------|--------------------------------------------|\n",
    "| Language Support     | Java-only (MapReduce), SQL (Hive)   | Scala, Java, Python, R                     |\n",
    "| Machine Learning     | Very slow (iterative jobs to disk)  | Fast, in-memory MLlib                      |\n",
    "| Streaming            | Not supported (batch only)          | Near real-time (Spark Streaming)           |\n",
    "| API Flexibility      | Rigid Map ‚Üí Reduce only             | Rich APIs (map, filter, join, SQL, ML, etc)|\n",
    "\n",
    "üëâ Spark broadened big data beyond Java engineers ‚Üí accessible to **analysts, data scientists, and ML engineers**.\n",
    "---\n",
    "\n",
    "## üîπ 4. Storage Limitations\n",
    "\n",
    "**Problem with Hadoop (HDFS)**  \n",
    "- HDFS storage was tightly coupled with compute.  \n",
    "- To add storage, you had to add new Hadoop nodes (CPU + RAM you might not need).  \n",
    "- With the rise of cloud, HDFS became costly compared to **Amazon S3, Azure Blob, GCS**.  \n",
    "\n",
    "**Spark‚Äôs Solution**  \n",
    "- Spark decoupled compute from storage.  \n",
    "- Works with both **HDFS** and **cloud storage**.  \n",
    "- Companies could adopt **cloud-native architectures (Lakehouse, Data Lake)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Resource Management\n",
    "\n",
    "**Problem with Hadoop (YARN)**  \n",
    "- YARN was the only resource manager.  \n",
    "- Felt heavy compared to modern options (Docker, Mesos, Kubernetes).  \n",
    "- Limited flexibility for cloud environments.  \n",
    "\n",
    "**Spark‚Äôs Solution**  \n",
    "- Spark is **resource-manager agnostic**.  \n",
    "- Can run on **YARN, Mesos, Kubernetes, or standalone clusters**.  \n",
    "- Fits naturally into **cloud + container-native ecosystems**.  \n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Summary\n",
    "\n",
    "| Problem in Hadoop/Hive | Spark‚Äôs Solution |\n",
    "|-------------------------|------------------|\n",
    "| Slow (disk I/O bottlenecks) | In-memory computing ‚Üí 10‚Äì100x faster |\n",
    "| Complex MapReduce programming | High-level APIs + Spark SQL |\n",
    "| Java-only, limited flexibility | Multi-language (Scala, Python, R, Java) |\n",
    "| HDFS tied to compute nodes | Works with HDFS + cloud storage (S3, Blob, GCS) |\n",
    "| YARN-only resource manager | Runs on YARN, Mesos, Kubernetes, standalone |\n",
    "\n",
    "üëâ Hadoop was **revolutionary** for big data,  \n",
    "but Spark made it **faster, simpler, and cloud-ready**,  \n",
    "which is why Spark is now the dominant big data engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5317",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76a4a4e2",
   "metadata": {},
   "source": [
    "# Data Lakes\n",
    "\n",
    "Data Lakes emerged as an evolution in distributed computing and storage. Let‚Äôs break down their history, functionality, and architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Origins of Data Lakes\n",
    "\n",
    "- It started with **Google File System (GFS)** to solve massive storage needs.  \n",
    "- The open-source community created **HDFS (Hadoop Distributed File System)**.  \n",
    "- Alongside storage, we got **MapReduce** to harness **cluster computing power** for large-scale data processing.  \n",
    "\n",
    "üëâ This allowed organizations to use **regular servers** instead of expensive supercomputers.\n",
    "\n",
    "![Hadoop Evolution](./images/hadoop.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üèõÔ∏è Before Data Lakes: Data Warehouses\n",
    "\n",
    "- Systems like **Teradata** and **Exadata** collected OLTP data into **structured warehouses**.  \n",
    "- Used for **reporting and business insights**.  \n",
    "- Challenges with warehouses:  \n",
    "  - Expensive & complex **vertical scaling**  \n",
    "  - Needed **capacity planning upfront**  \n",
    "  - Mostly supported **structured data** only  \n",
    "\n",
    "---\n",
    "\n",
    "## üåä Rise of Data Lakes\n",
    "\n",
    "HDFS + MapReduce (and later **Spark**) began to challenge warehouses:\n",
    "\n",
    "1. **Scaling**: Just add more cheap servers (**horizontal scaling**).  \n",
    "2. **Cost**: Start small, expand as needed (low capital investment).  \n",
    "3. **Data formats**: Support for **structured, semi-structured, and unstructured data** (text, JSON, XML, images, audio, video).  \n",
    "\n",
    "üëâ To describe this new paradigm, **James Dixon (CTO of Pentaho)** coined the term **Data Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How Data Lakes Work\n",
    "\n",
    "Like warehouses, data is collected from multiple sources into **HDFS** or cloud object stores.  \n",
    "Processing is done using **MapReduce** or (today) **Apache Spark**.  \n",
    "Processed outputs are stored back in the lake for:  \n",
    "- **BI & Reporting**  \n",
    "- **Data Science**  \n",
    "- **Machine Learning / AI**  \n",
    "\n",
    "![Data Lake Architecture](./images/data_lake.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üõë Initial Problems with Data Lakes\n",
    "\n",
    "Compared to warehouses, early Data Lakes lacked:\n",
    "\n",
    "- **Transactions & Consistency**  \n",
    "- **High-performance reporting**  \n",
    "\n",
    "### Solution: Hybrid Approach\n",
    "- Store raw + processed data in the **Data Lake**  \n",
    "- Push transformed, query-optimized data into a **Data Warehouse** for BI/reporting  \n",
    "- Keep **ML/AI workloads** running directly on the Data Lake  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Modern Data Lake Capabilities\n",
    "\n",
    "Over time, Data Lakes matured with **four key capabilities**:\n",
    "\n",
    "1. **Data Collection & Ingestion**  \n",
    "   - Bring raw, immutable copies of data from various sources.  \n",
    "   - Tools vary (Kafka, Flume, cloud ingestion services).  \n",
    "\n",
    "2. **Data Storage & Management**  \n",
    "   - Storage backbone = HDFS or cloud object stores (Amazon S3, Azure Blob, Google Cloud Storage).  \n",
    "   - Cloud storage preferred for **scale + availability + low cost**.  \n",
    "\n",
    "3. **Data Processing & Transformation**  \n",
    "   - Compute layer handles:  \n",
    "     - Quality checks  \n",
    "     - Transformations  \n",
    "     - Aggregations & analytics  \n",
    "     - Machine Learning models  \n",
    "   - Dominated by **Apache Spark** today.  \n",
    "\n",
    "4. **Data Access & Retrieval**  \n",
    "   - Consumers include BI dashboards, analysts, ML models, and applications.  \n",
    "   - Multiple interfaces: JDBC/ODBC, REST APIs, file download, search, Spark connectors.  \n",
    "   - Different users ‚Üí different expectations.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîê Additional Critical Capabilities\n",
    "\n",
    "To be production-ready, modern Data Lakes must also support:\n",
    "\n",
    "- **Security & Access Control**  \n",
    "- **Scheduling & Workflow Management**  \n",
    "- **Data Catalog & Metadata Management**  \n",
    "- **Data Lifecycle & Governance**  \n",
    "- **Monitoring & Operations**  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Summary\n",
    "\n",
    "- **Data Warehouses**: Optimized for **structured, query-ready** data.  \n",
    "- **Data Lakes**: Handle **raw, structured, semi-structured, and unstructured** data at scale.  \n",
    "- **Lakehouse architectures** combine both worlds (BI on warehouses, ML/AI on lakes).  \n",
    "\n",
    "üëâ Today‚Äôs Data Lakes = a flexible, scalable foundation for **analytics, machine learning, and enterprise data platforms**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702d16c",
   "metadata": {},
   "source": [
    "# Apache Spark Overview\n",
    "\n",
    "![Spark Ecosystem](./images/spark_table.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is Apache Spark?\n",
    "Apache Spark is a **distributed data processing framework**. It‚Äôs designed to handle very large datasets by splitting them across a **cluster of machines** and processing them in **parallel**.\n",
    "\n",
    "Key points:\n",
    "- Spark focuses only on **computation**.\n",
    "- It does **not manage storage** (data lives in HDFS, S3, ADLS, GCS, etc.).\n",
    "- It does **not manage cluster resources** (relies on YARN, Kubernetes, Mesos, or Spark Standalone).\n",
    "- Spark provides a **compute engine** that runs jobs efficiently (mostly in memory) with **fault tolerance**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Storage Layer\n",
    "- Spark does not have its own storage.\n",
    "- Reads/writes from:\n",
    "  - **HDFS** (Hadoop Distributed File System)\n",
    "  - **Cloud storage** (Amazon S3, Azure Data Lake, Google Cloud Storage)\n",
    "  - **Databases/NoSQL** (JDBC sources, Cassandra, etc.)\n",
    "- Best with columnar formats:\n",
    "  - **Parquet / ORC** ‚Üí columnar, compressed, splittable\n",
    "  - **Partition pruning**, **predicate pushdown**, **projection pushdown** reduce I/O\n",
    "\n",
    "üëâ Storage is the **fridge** with ingredients; Spark is the **chef** that fetches what it needs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Cluster Manager Layer\n",
    "Spark needs a **resource allocator** to run on a cluster:\n",
    "- **YARN** (common in Hadoop)\n",
    "- **Kubernetes** (modern container orchestration)\n",
    "- **Standalone** (Spark‚Äôs simple built-in manager)\n",
    "- **Mesos** (less common now)\n",
    "\n",
    "üëâ The cluster manager is like **HR/admin** providing machines (executors), CPUs, and memory.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Spark Compute Engine (the ‚Äúbrain‚Äù)\n",
    "- Builds a **DAG (Directed Acyclic Graph)** from your transformations.\n",
    "- Splits a **Job** into **Stages** at shuffle boundaries.\n",
    "- Splits each Stage into **Tasks** (1 per partition).\n",
    "- Schedules tasks on executors (tries **data locality** when possible).\n",
    "- **Fault tolerance** via lineage (recompute lost partitions), retries failed tasks, monitors progress.\n",
    "\n",
    "üëâ Think **chef** turning a recipe into steps and delegating them to helpers.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Core APIs (RDD APIs)\n",
    "- **RDD = Resilient Distributed Dataset**: immutable, partitioned, fault-tolerant, lazy.\n",
    "- Operations:\n",
    "  - **Transformations** (`map`, `filter`, `reduceByKey`) ‚Üí build new RDDs (lazy)\n",
    "  - **Actions** (`count`, `collect`, `saveAsTextFile`) ‚Üí trigger execution (jobs)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "squared = rdd.map(lambda x: x**2)\n",
    "print(squared.collect())  # [1, 4, 9, 16]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ High-level APIs & DSLs\n",
    "Built on top of RDDs to be easier and faster (auto-optimized by Catalyst/Tungsten):\n",
    "\n",
    "1) **Spark SQL & DataFrames**\n",
    "```python\n",
    "df = spark.read.parquet(\"s3://bucket/events/\")\n",
    "df.filter(df[\"country\"] == \"IN\").groupBy(\"user_id\").count().show()\n",
    "```\n",
    "\n",
    "2) **Structured Streaming** (Kafka/logs ‚Üí real-time tables)\n",
    "\n",
    "3) **MLlib** (ML algorithms on DataFrames)\n",
    "\n",
    "4) **GraphX / GraphFrames** (graph algorithms like PageRank)\n",
    "\n",
    "üëâ These are your **kitchen appliances** (blender, rice cooker) vs chopping everything by hand (raw RDDs).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ End-to-End Flow\n",
    "1. You write code (SQL/DataFrame/RDD).\n",
    "2. Spark builds a **DAG** of transformations.\n",
    "3. An **Action** triggers a **Job**.\n",
    "4. Job ‚Üí split into **Stages** (by shuffles).\n",
    "5. Stage ‚Üí split into **Tasks** (one per partition).\n",
    "6. **Executors** (via the cluster manager) run tasks in parallel.\n",
    "7. Results return to the driver or are written back to storage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a6d4a",
   "metadata": {},
   "source": [
    "# Why Apache Spark? \n",
    "\n",
    "![Why Spark](./images/why_spark.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Abstraction\n",
    "Spark **hides the complexity of distributed systems** so you can focus on logic, not plumbing.\n",
    "- Code against **tables (SQL)** or **collections (DataFrames/RDDs)**; Spark handles **parallelism, scheduling, retries, locality, memory/disk**.\n",
    "- You write transformations; the **engine builds a DAG**, splits into stages/tasks, and runs it across the cluster.\n",
    "- Feels like using a **database** (SQL) or **Pandas/Scala collections**, while Spark manages cluster execution underneath.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Unified Platform\n",
    "One framework for many data problems‚Äîmix & match in a single app.\n",
    "- **SQL & DataFrames/Datasets** for structured/semi-structured data.\n",
    "- **Batch ETL** and **Structured Streaming** (continuous, unbounded data).\n",
    "- **MLlib / DL integrations** for machine learning and AI.\n",
    "- **GraphX / GraphFrames** for graph algorithms.\n",
    "- Multi-language: **Scala, Java, Python, R**.  \n",
    "- Runs anywhere: **YARN, Kubernetes, Standalone, Mesos**.  \n",
    "- Reads/writes: **HDFS, S3, ADLS, GCS, Cassandra**, JDBC, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Ease of Use\n",
    "Simpler, shorter, and faster than old Hadoop MapReduce.\n",
    "- Concise, high-level APIs; **less boilerplate** than MR.\n",
    "- **Catalyst** (query optimizer) + **Tungsten** (efficient execution/memory) give strong performance.\n",
    "- Rich, evolving **ecosystem & libraries**; integrates with many tools.\n",
    "- Write locally; **scale out** to clusters with minimal code changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Compare (Why it‚Äôs popular)\n",
    "- **MapReduce:** disk-heavy intermediates, verbose code, slower for iterative/interactive workloads.  \n",
    "- **Spark:** keeps intermediates **in memory when beneficial**, spills if needed, optimizes end-to-end ‚Üí typically **10‚Äì100√ó faster** and much **easier to develop**.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "Spark is popular because it **abstracts distributed complexity**, **unifies** batch/streaming/ML/graph in one platform, and is **easy to use**‚Äîletting you solve real data problems quickly and at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d1083",
   "metadata": {},
   "source": [
    "# Databricks \n",
    "\n",
    "![Databricks Overview](./images/databricks.png)\n",
    "\n",
    "---\n",
    "\n",
    "## What is Databricks?\n",
    "Databricks is a **commercial platform** built by the original creators of Apache Spark. Spark remains **open-source**; Databricks wraps it with cloud-native tooling that makes running Spark **easier, faster, and managed**.\n",
    "\n",
    "---\n",
    "\n",
    "## Where does it run?\n",
    "- Available on **AWS**, **Azure**, and **Google Cloud**.\n",
    "- Brings ‚Äú**Spark on the Cloud**‚Äù with first-class integrations to each provider.\n",
    "\n",
    "---\n",
    "\n",
    "## What does Databricks add on top of Apache Spark?\n",
    "\n",
    "1. **Spark Cluster Management**\n",
    "   - Launch/resize/terminate clusters with a few clicks.\n",
    "   - Handles dependencies and runtimes across all nodes.\n",
    "\n",
    "2. **Notebooks & Workspace**\n",
    "   - Collaborative notebooks (an IDE-like experience) for development and sharing.\n",
    "   - Git integration for version control.\n",
    "\n",
    "3. **Administration Controls**\n",
    "   - Workspace/cluster/storage access management and security guardrails.\n",
    "\n",
    "4. **Optimized Spark Runtime**\n",
    "   - Tuned Spark distribution (stated as *up to ~5√ó faster* than vanilla Spark in the transcript context).\n",
    "\n",
    "5. **Databases/Tables & Catalog**\n",
    "   - Integrated Hive metastore for **databases, tables, and views** managed via Spark SQL.\n",
    "\n",
    "6. **Databricks SQL (Photon)**\n",
    "   - Advanced SQL engine (‚Äú**Photon**‚Äù) that brings **data-warehouse-grade** performance for queries and dashboards on data lake storage.\n",
    "\n",
    "7. **Delta Lake Integration**\n",
    "   - Native **ACID transactions** and data consistency on lake storage; reliable upserts, schema evolution, and time travel.\n",
    "\n",
    "8. **MLflow**\n",
    "   - End-to-end **ML lifecycle** management: experiments, models, deployments, and model registry.\n",
    "\n",
    "9. **Industry Vertical Accelerators**\n",
    "   - Prebuilt solution patterns to speed up common, domain-specific use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Why teams choose Databricks\n",
    "- **Simplicity:** No manual cluster wrangling; faster setup and iteration.\n",
    "- **Performance:** Optimized runtimes + Photon + Delta Lake.\n",
    "- **Unified:** One platform spanning **SQL, batch, streaming, ML, and governance**.\n",
    "- **Cloud-native:** Seamless integration with services across AWS/Azure/GCP.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
