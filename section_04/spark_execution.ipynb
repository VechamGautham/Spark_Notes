{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b74ff73",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Executing spark programs in apache spark](#executing-spark-programs-in-apache-spark)\n",
    "    - [Interactve clients](#interactive-clients)\n",
    "    - [Spark Submit](#spark-submit)\n",
    "- [Spark on Yarn (master slave architecture)](#spark-on-yarn-driverexecutor-flow)\n",
    "- [How spark runs (local,interactive client and cluster)](#how-spark-runs-local-interactive-client-and-cluster-mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece11767",
   "metadata": {},
   "source": [
    "# Executing Spark Programs in Apache Spark\n",
    "\n",
    "There are two primary ways to run Spark code: **Interactive Clients** and **`spark-submit`**. Use interactive tools for learning/exploration, and `spark-submit` for production-grade jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Clients\n",
    "\n",
    "**What it is**\n",
    "- Tools like **Spark Shell** (Scala/PySpark REPL) and **Notebooks** (Jupyter, Databricks) where you run code **line by line**.\n",
    "\n",
    "**Why**\n",
    "- Fast feedback loop for **learning**, **data exploration**, and **debugging**.\n",
    "\n",
    "**When**\n",
    "- During **development** or when prototyping logic before packaging an application.\n",
    "\n",
    "**Example (PySpark shell)**\n",
    "    \n",
    "    $ pyspark\n",
    "    # Inside the shell:\n",
    "    df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "    df.groupBy(\"category\").count().show()\n",
    "\n",
    "**Example (Notebook cell)**\n",
    "    \n",
    "    df = spark.read.parquet(\"/mnt/raw/events/\")\n",
    "    df.select(\"user_id\", \"event_type\").where(df.event_type == \"purchase\").display()\n",
    "\n",
    "---\n",
    "\n",
    "## spark-submit\n",
    "\n",
    "**What it is**\n",
    "- A **universal CLI tool** to package and send your Spark application to a cluster (local, YARN, k8s, Standalone).\n",
    "\n",
    "**Why**\n",
    "- Suitable for **production**: scheduled **batch** jobs and **streaming** apps with resource configs, retries, and logs handled by the cluster.\n",
    "\n",
    "**When**\n",
    "- After prototyping is done; for **automated**, **repeatable**, and **scalable** execution.\n",
    "\n",
    "**Example (submit a Python app locally)**\n",
    "    \n",
    "    spark-submit \\\n",
    "      --master local[*] \\\n",
    "      my_app.py\n",
    "\n",
    "**Example (submit to YARN in cluster mode)**\n",
    "    \n",
    "    spark-submit \\\n",
    "      --master yarn \\\n",
    "      --deploy-mode cluster \\\n",
    "      --conf spark.executor.memory=4g \\\n",
    "      --conf spark.executor.cores=2 \\\n",
    "      jobs/daily_agg.py\n",
    "\n",
    "**Minimal Python app structure**\n",
    "    \n",
    "    # my_app.py\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"DailyAgg\").getOrCreate()\n",
    "    df = spark.read.parquet(\"s3://bucket/input/\")\n",
    "    out = df.groupBy(\"category\").count()\n",
    "    out.write.mode(\"overwrite\").parquet(\"s3://bucket/output/daily_agg/\")\n",
    "    spark.stop()\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Quick Summary\n",
    "\n",
    "- **Interactive Clients** → best for **exploration & development**; immediate results, iterate quickly.  \n",
    "- **`spark-submit`** → best for **production**; packaged jobs (batch or streaming) running reliably on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a6cc6",
   "metadata": {},
   "source": [
    "# Spark on YARN Driver/Executor Flow\n",
    "\n",
    "This walk-through explains **what happens after you submit a Spark app** to **YARN / the Cluster Manager** and how the **Driver (master)** and **Executors (slaves)** are created and interact.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Submit\n",
    "You run `spark-submit --master yarn ...`. The submission is sent to the **YARN ResourceManager (RM)**.\n",
    "\n",
    "![Submission enters the cluster via RM](./images/a1_application.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Launch the Driver (via ApplicationMaster)\n",
    "- **RM** selects a **NodeManager (NM)** with capacity and **launches one container** for your application’s **ApplicationMaster (AM)**.  \n",
    "- For **Spark on YARN (cluster mode)**, the **Driver runs inside this AM container** (i.e., the AM hosts the Driver process).\n",
    "- The Driver initializes the **SparkSession/SparkContext**, registers with YARN, and starts building the job DAG.\n",
    "\n",
    "🔹 Important Clarification\n",
    "\t•\tThe AM is not separate from the Driver in Spark-on-YARN (cluster mode).\n",
    "\t•\tThe first container hosts both AM + Driver.\n",
    "\t•\tThe AM’s only job is to talk to RM for resource requests (executors).\n",
    "\t•\tOnce the Driver is running, it coordinates all Executors directly.\n",
    "\t•\tExecutors are always in separate containers (one container = one executor).\n",
    "\n",
    "![A1 Driver starts inside the cluster](./images/driver_process_a1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Ask for Executor Containers\n",
    "- The **Driver (through the AM)** requests more containers from **RM** for **Executors** (each executor needs its own CPU/memory spec).\n",
    "- **RM** allocates containers across available **NodeManagers**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Launch Executors (one per container)\n",
    "- Each allocated container is used to **start one Executor JVM**.\n",
    "- Executors register back with the **Driver** and are ready to run tasks.\n",
    "- **Important:** there is **one container for the Driver** and **one container per Executor** (Executors are *not* colocated in the Driver’s container).\n",
    "\n",
    "![Driver with multiple Executors (each executor in its own container)](./images/a1_executor.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Schedule & Execute Tasks\n",
    "- The **Driver** splits your job into **stages → tasks** based on the DAG (and shuffles).  \n",
    "- Tasks are assigned to **Executors**, preferring **data locality** when possible.  \n",
    "- Executors:\n",
    "  - Run tasks on their **partitions** (map/filter/joins/aggregations).  \n",
    "  - Materialize shuffle outputs when needed.  \n",
    "  - **Report status & metrics** back to the Driver.  \n",
    "- On failures, the Driver can **relaunch tasks** or **request new containers** for lost executors.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Completion & Cleanup\n",
    "- When jobs finish:\n",
    "  - Executors flush/write results (e.g., HDFS/S3/Delta/DB).\n",
    "  - **Driver** stops, **AM** unregisters, and **RM** **releases all containers**.\n",
    "  - Logs/metrics remain available via YARN UIs / Spark UI (per app).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Multiple Independent Apps\n",
    "Each Spark application has **its own Driver + Executors set**, isolated from others. Submitting a second app (A2) repeats the same flow with **separate** containers.\n",
    "\n",
    "![A1 and A2 each have their own Driver and Executors](./images/a1_a2_application.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Roles Recap\n",
    "- **YARN (RM/NM/AM)** = **Resource orchestration** (containers, placement, lifecycle).  \n",
    "- **Spark Driver** = **Coordinator / “master”** (DAG, scheduling, task assignment, app state).  \n",
    "- **Spark Executors** = **Workers / “slaves”** (run tasks, cache/shuffle data, report to Driver).\n",
    "\n",
    "**Rule of thumb:** *1 Driver container* + *N Executor containers* per Spark app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0a51d",
   "metadata": {},
   "source": [
    "# How Spark Runs: Local, Interactive Client, and Cluster Mode\n",
    "\n",
    "Spark can run in different environments depending on whether you are testing locally, exploring interactively, or submitting production jobs to a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Spark Local Mode\n",
    "\n",
    "**What it is**\n",
    "- Runs entirely on a single machine; no real cluster manager.\n",
    "- Configure with `local[n]`, where `n` is the number of threads.\n",
    "\n",
    "**How it behaves**\n",
    "- `local[1]` → Driver only (no executors); work runs serially.\n",
    "- `local[n]` (n > 1) → Driver + multiple executor **threads** (parallelism simulated with threads).\n",
    "\n",
    "**Best for**\n",
    "- Quick development, debugging, unit tests.\n",
    "\n",
    "**Example master config**\n",
    "- `local[3]` (one driver thread + two executor threads)\n",
    "\n",
    "**Diagrams**\n",
    "![Local with 1 thread (Driver only)](./images/local_1.png)\n",
    "![Local with 3 threads (Driver + Executor threads)](./images/local_3.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Interactive Client Mode (spark-shell / notebooks)\n",
    "\n",
    "**What it is**\n",
    "- The **Driver runs on your client machine** (laptop/IDE/notebook server).\n",
    "- The Driver talks to the **cluster manager** (YARN/Kubernetes/Standalone) to start **Executors on the cluster**.\n",
    "\n",
    "**Important**\n",
    "- If the **client logs off or dies**, the **Driver dies**, and **all executors terminate** because they depend on the Driver.\n",
    "\n",
    "**Best for**\n",
    "- Ad-hoc analysis, exploration, iterative development.\n",
    "\n",
    "**Diagrams**\n",
    "![Client mode: Driver on client, Executors on cluster](./images/client_mode.png)\n",
    "![If client/driver dies, executors die](./images/client_mode_dies.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Cluster Mode (production submissions)\n",
    "\n",
    "**What it is**\n",
    "- You submit a packaged app; the **Driver runs inside the cluster** (in a container), and **each Executor runs in its own container** on worker nodes.\n",
    "\n",
    "**Why it matters**\n",
    "- After submission, you can disconnect; the job continues to run.\n",
    "- Preferred for **batch** and **streaming** production workloads.\n",
    "\n",
    "**Diagram**\n",
    "![Cluster mode: Driver and Executors inside the cluster](./images/cluster_mode.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Quick Summary\n",
    "\n",
    "- **Local mode** → Single machine; `local[1]` = driver only; `local[n]` = driver + executor threads.  \n",
    "- **Interactive client mode** → Driver on client, executors on cluster; **if client logs off, the whole Spark app stops**.  \n",
    "- **Cluster mode** → Driver + executors both in the cluster; safe to disconnect; ideal for production."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
