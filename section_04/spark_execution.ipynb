{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b74ff73",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Executing spark programs in apache spark](#executing-spark-programs-in-apache-spark)\n",
    "    - [Interactve clients](#interactive-clients)\n",
    "    - [Spark Submit](#spark-submit)\n",
    "- [Spark on Yarn (master slave architecture)](#spark-on-yarn-driverexecutor-flow)\n",
    "- [How spark runs (local,interactive client and cluster)](#how-spark-runs-local-interactive-client-and-cluster-mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece11767",
   "metadata": {},
   "source": [
    "# Executing Spark Programs in Apache Spark\n",
    "\n",
    "There are two primary ways to run Spark code: **Interactive Clients** and **`spark-submit`**. Use interactive tools for learning/exploration, and `spark-submit` for production-grade jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Clients\n",
    "\n",
    "**What it is**\n",
    "- Tools like **Spark Shell** (Scala/PySpark REPL) and **Notebooks** (Jupyter, Databricks) where you run code **line by line**.\n",
    "\n",
    "**Why**\n",
    "- Fast feedback loop for **learning**, **data exploration**, and **debugging**.\n",
    "\n",
    "**When**\n",
    "- During **development** or when prototyping logic before packaging an application.\n",
    "\n",
    "**Example (PySpark shell)**\n",
    "    \n",
    "    $ pyspark\n",
    "    # Inside the shell:\n",
    "    df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "    df.groupBy(\"category\").count().show()\n",
    "\n",
    "**Example (Notebook cell)**\n",
    "    \n",
    "    df = spark.read.parquet(\"/mnt/raw/events/\")\n",
    "    df.select(\"user_id\", \"event_type\").where(df.event_type == \"purchase\").display()\n",
    "\n",
    "---\n",
    "\n",
    "## spark-submit\n",
    "\n",
    "**What it is**\n",
    "- A **universal CLI tool** to package and send your Spark application to a cluster (local, YARN, k8s, Standalone).\n",
    "\n",
    "**Why**\n",
    "- Suitable for **production**: scheduled **batch** jobs and **streaming** apps with resource configs, retries, and logs handled by the cluster.\n",
    "\n",
    "**When**\n",
    "- After prototyping is done; for **automated**, **repeatable**, and **scalable** execution.\n",
    "\n",
    "**Example (submit a Python app locally)**\n",
    "    \n",
    "    spark-submit \\\n",
    "      --master local[*] \\\n",
    "      my_app.py\n",
    "\n",
    "**Example (submit to YARN in cluster mode)**\n",
    "    \n",
    "    spark-submit \\\n",
    "      --master yarn \\\n",
    "      --deploy-mode cluster \\\n",
    "      --conf spark.executor.memory=4g \\\n",
    "      --conf spark.executor.cores=2 \\\n",
    "      jobs/daily_agg.py\n",
    "\n",
    "**Minimal Python app structure**\n",
    "    \n",
    "    # my_app.py\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"DailyAgg\").getOrCreate()\n",
    "    df = spark.read.parquet(\"s3://bucket/input/\")\n",
    "    out = df.groupBy(\"category\").count()\n",
    "    out.write.mode(\"overwrite\").parquet(\"s3://bucket/output/daily_agg/\")\n",
    "    spark.stop()\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick Summary\n",
    "\n",
    "- **Interactive Clients** â†’ best for **exploration & development**; immediate results, iterate quickly.  \n",
    "- **`spark-submit`** â†’ best for **production**; packaged jobs (batch or streaming) running reliably on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a6cc6",
   "metadata": {},
   "source": [
    "# Spark on YARN Driver/Executor Flow\n",
    "\n",
    "This walk-through explains **what happens after you submit a Spark app** to **YARN / the Cluster Manager** and how the **Driver (master)** and **Executors (slaves)** are created and interact.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Submit\n",
    "You run `spark-submit --master yarn ...`. The submission is sent to the **YARN ResourceManager (RM)**.\n",
    "\n",
    "![Submission enters the cluster via RM](./images/a1_application.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Launch the Driver (via ApplicationMaster)\n",
    "- **RM** selects a **NodeManager (NM)** with capacity and **launches one container** for your applicationâ€™s **ApplicationMaster (AM)**.  \n",
    "- For **Spark on YARN (cluster mode)**, the **Driver runs inside this AM container** (i.e., the AM hosts the Driver process).\n",
    "- The Driver initializes the **SparkSession/SparkContext**, registers with YARN, and starts building the job DAG.\n",
    "\n",
    "ðŸ”¹ Important Clarification\n",
    "\tâ€¢\tThe AM is not separate from the Driver in Spark-on-YARN (cluster mode).\n",
    "\tâ€¢\tThe first container hosts both AM + Driver.\n",
    "\tâ€¢\tThe AMâ€™s only job is to talk to RM for resource requests (executors).\n",
    "\tâ€¢\tOnce the Driver is running, it coordinates all Executors directly.\n",
    "\tâ€¢\tExecutors are always in separate containers (one container = one executor).\n",
    "\n",
    "![A1 Driver starts inside the cluster](./images/driver_process_a1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Ask for Executor Containers\n",
    "- The **Driver (through the AM)** requests more containers from **RM** for **Executors** (each executor needs its own CPU/memory spec).\n",
    "- **RM** allocates containers across available **NodeManagers**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Launch Executors (one per container)\n",
    "- Each allocated container is used to **start one Executor JVM**.\n",
    "- Executors register back with the **Driver** and are ready to run tasks.\n",
    "- **Important:** there is **one container for the Driver** and **one container per Executor** (Executors are *not* colocated in the Driverâ€™s container).\n",
    "\n",
    "![Driver with multiple Executors (each executor in its own container)](./images/a1_executor.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Schedule & Execute Tasks\n",
    "- The **Driver** splits your job into **stages â†’ tasks** based on the DAG (and shuffles).  \n",
    "- Tasks are assigned to **Executors**, preferring **data locality** when possible.  \n",
    "- Executors:\n",
    "  - Run tasks on their **partitions** (map/filter/joins/aggregations).  \n",
    "  - Materialize shuffle outputs when needed.  \n",
    "  - **Report status & metrics** back to the Driver.  \n",
    "- On failures, the Driver can **relaunch tasks** or **request new containers** for lost executors.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Completion & Cleanup\n",
    "- When jobs finish:\n",
    "  - Executors flush/write results (e.g., HDFS/S3/Delta/DB).\n",
    "  - **Driver** stops, **AM** unregisters, and **RM** **releases all containers**.\n",
    "  - Logs/metrics remain available via YARN UIs / Spark UI (per app).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Multiple Independent Apps\n",
    "Each Spark application has **its own Driver + Executors set**, isolated from others. Submitting a second app (A2) repeats the same flow with **separate** containers.\n",
    "\n",
    "![A1 and A2 each have their own Driver and Executors](./images/a1_a2_application.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§­ Roles Recap\n",
    "- **YARN (RM/NM/AM)** = **Resource orchestration** (containers, placement, lifecycle).  \n",
    "- **Spark Driver** = **Coordinator / â€œmasterâ€** (DAG, scheduling, task assignment, app state).  \n",
    "- **Spark Executors** = **Workers / â€œslavesâ€** (run tasks, cache/shuffle data, report to Driver).\n",
    "\n",
    "**Rule of thumb:** *1 Driver container* + *N Executor containers* per Spark app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0a51d",
   "metadata": {},
   "source": [
    "# How Spark Runs: Local, Interactive Client, and Cluster Mode\n",
    "\n",
    "Spark can run in different environments depending on whether you are testing locally, exploring interactively, or submitting production jobs to a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Spark Local Mode\n",
    "\n",
    "**What it is**\n",
    "- Runs entirely on a single machine; no real cluster manager.\n",
    "- Configure with `local[n]`, where `n` is the number of threads.\n",
    "\n",
    "**How it behaves**\n",
    "- `local[1]` â†’ Driver only (no executors); work runs serially.\n",
    "- `local[n]` (n > 1) â†’ Driver + multiple executor **threads** (parallelism simulated with threads).\n",
    "\n",
    "**Best for**\n",
    "- Quick development, debugging, unit tests.\n",
    "\n",
    "**Example master config**\n",
    "- `local[3]` (one driver thread + two executor threads)\n",
    "\n",
    "**Diagrams**\n",
    "![Local with 1 thread (Driver only)](./images/local_1.png)\n",
    "![Local with 3 threads (Driver + Executor threads)](./images/local_3.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Interactive Client Mode (spark-shell / notebooks)\n",
    "\n",
    "**What it is**\n",
    "- The **Driver runs on your client machine** (laptop/IDE/notebook server).\n",
    "- The Driver talks to the **cluster manager** (YARN/Kubernetes/Standalone) to start **Executors on the cluster**.\n",
    "\n",
    "**Important**\n",
    "- If the **client logs off or dies**, the **Driver dies**, and **all executors terminate** because they depend on the Driver.\n",
    "\n",
    "**Best for**\n",
    "- Ad-hoc analysis, exploration, iterative development.\n",
    "\n",
    "**Diagrams**\n",
    "![Client mode: Driver on client, Executors on cluster](./images/client_mode.png)\n",
    "![If client/driver dies, executors die](./images/client_mode_dies.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Cluster Mode (production submissions)\n",
    "\n",
    "**What it is**\n",
    "- You submit a packaged app; the **Driver runs inside the cluster** (in a container), and **each Executor runs in its own container** on worker nodes.\n",
    "\n",
    "**Why it matters**\n",
    "- After submission, you can disconnect; the job continues to run.\n",
    "- Preferred for **batch** and **streaming** production workloads.\n",
    "\n",
    "**Diagram**\n",
    "![Cluster mode: Driver and Executors inside the cluster](./images/cluster_mode.png)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick Summary\n",
    "\n",
    "- **Local mode** â†’ Single machine; `local[1]` = driver only; `local[n]` = driver + executor threads.  \n",
    "- **Interactive client mode** â†’ Driver on client, executors on cluster; **if client logs off, the whole Spark app stops**.  \n",
    "- **Cluster mode** â†’ Driver + executors both in the cluster; safe to disconnect; ideal for production."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
